{
  
    
        "post0": {
            "title": "Optimizing Risks for a Portfolio of Cryptocurrencies",
            "content": "#install requirements if needed !pip install cvxpy==1.0.31 pandas_datareader pandas matplotlib seaborn -q . #import modules import cvxpy as cp import pandas_datareader as pdr import numpy as np import matplotlib.pyplot as plt import pandas as pd import math import os.path import time #select plotting style import matplotlib import seaborn as sb matplotlib.style.use(&#39;fivethirtyeight&#39;) #seaborn-white fivethirtyeight import sys #Python versiond print(sys.version) print(cp.__version__) . #read input dataset that was downladed from the binance fuldf = pd.read_pickle(&#39;data.pickle&#39;) y_label = fuldf.columns[0] factors = (fuldf.columns[1:] .tolist() ) total_size = fuldf.shape[0] # print(&#39;Combined DF size:&#39;, total_size,&#39; nFull range: &#39;, fuldf.iloc[0].name,fuldf.iloc[-1].name) train_set = .8 train_id = int( total_size * train_set) df_train = fuldf.iloc[:train_id] df_test = fuldf.iloc[train_id:] split_index = fuldf.iloc[train_id].name # print(&#39;Training period: &#39;, df_train.iloc[0].name,&#39;-&#39;,df_train.iloc[-1].name,&#39;shape:&#39;,df_train.shape[0],&#39; nTesing period:&#39;,df_test.iloc[0].name,df_test.iloc[-1].name,&#39;shape:&#39;,df_test.shape[0],) . Data Acquisition . The data for this exercise was obtained from the Binance API using the Python API client python-binance. . An example of the data acquisition code: . Note: Install the module if needed using pip: &gt; #pip install python-binance . from binance.client import Client binance_api_key = &#39;YOUR-API-KEY&#39; binance_api_secret = &#39;YOUR-API-SECRET&#39; binance_client = Client(api_key=binance_api_key ,api_secret=binance_api_secret) klines = binance_client.get_historical_klines(symbol ,kline_size, date_from, date_to) data = pd.DataFrame(klines, columns = [COLUMNS]) . Following daily pairs was downloaded and formatted to pd DataFrame:1. BTCUSDT2. ETHUSDT . BNBUSDT | LTCUSDT | For each timepoint, Binance provides conventional OHLC (Open, High, Low, Close) and Volume data. In this exercise, we used only the Close column. It&#39;s possible, though to consider the combination of all five values and came up with a more reliable metric, for example: the weighted average price. . The joint time-series of four crypto assets looks the following: . df_train.head() . Each trading pair has a different amount of data available: The oldest tradable crypto instruments on Binance are BTCUSDT and ETHUSDT - data points are available from 2017-08-17. For BNBUSDT and LTCUSDT first trading days are 2017-11-06 and 2017-12-13, respectively. . We joined these time series together on the earliest common trading date: 2017-12-13 up to 2021-06-14. The full joint dataset size is equal to 1280 samples. . In order to properly infer the SDF, We split the dataset into training and testing sets at 2020-10-02. . Training interval: 2017-12-13 to 2020-10-02 (1024 data points). . &nbsp; . Below is the joint plot of the full dataset in the logarithmic scale. The dotted line represents the train-test split at 2020-10-02 . plot_data =np.log(fuldf) fig , ax = plt.subplots(figsize=(12,7)) # ax1 = ax.twinx() # plot_data.drop(y_label,axis=1).plot(ax=ax) plot_data.plot(ax=ax,alpha=0.8) ax.legend(loc=0) ax.set(title=&#39;Assets&#39;,xlabel=&#39;Date&#39;,ylabel=&#39;price, log scale&#39;); ax.axvline(split_index,color=&#39;grey&#39;, linestyle=&#39;--&#39;, lw=2); . #Transform raw data to log-return format: lret_data = np.log1p(df_train.pct_change()).dropna(axis=0,how=&#39;any&#39;) . II. Data Transformation . In order to be properly trained, input time series has to be transfored into the log-return format: . $r_t = ln{ frac{P_t}{P_{t-1}}}$ . Where $P_t$ is a price of the asset at time $t$. . The SDF model needs two vectors to perform the optimization: . Feature vector $I$ | Target vector $R$ | The $R$ vector contains values shifted to $t+1$ compared to the $I$ vector: . $R = begin{pmatrix} r_{1} vdots r_{t} end{pmatrix} I = begin{pmatrix} r_{0,0} &amp; dots &amp; r_{0,l} vdots &amp; &amp; vdots r_{t-1,0} &amp; dots &amp; r_{t-1,l} end{pmatrix} $ . Where $l$ is the number of features in the model. . R = lret_data[y_label].shift(1).iloc[1:].values.reshape((-1,1)) I = lret_data[factors].iloc[1:].values # assert R.shape[0] == I.shape[0] and I.shape[1]==3 print(&#39;R:&#39;,R.shape,&#39; n&#39;,R[:10] ,&#39; n I:&#39;,I.shape,&#39; n&#39;,I[:10,:]) . #correlation matrix F= pd.concat([pd.DataFrame(R),pd.DataFrame(I)],axis=1) F.columns = [y_label]+factors F.cov() . f = plt.figure(figsize=(10, 10)) cov_data = F.cov() mask = np.triu(np.ones_like(cov_data)) dataplot = sb.heatmap(cov_data.corr(), cmap=&quot;YlGnBu&quot;, annot=True, mask=mask) plt.title(&#39;Covariance Matrix&#39;, fontsize=16); . Model . Our goal is to explain the differences in the cross-section of returns $R$ for individual stocks. . Let $R_{t+1}$; denote the return of BTCUSDT at time $t + 1$. The fundamental no-arbitrage assumption is equivalent to the existence of a stochastic discount factor (SDF) $M_{t+1}$ such that for any return in excess of the risk-free rate . $R^e_{t+1} = R_{t+1}- R^f_{t+1}$ it holds: . $E_t[M_{t+1} R^e_{t+1}]= 0$ . For demonstration purpose and in seak of simplicity, we assume that $R^e_{t} = R_{t}$: excesses return of the BTCUSDT is the same as the market return. . We consider the SDF formulation as: . $M_{t+1} = 1 - sum_{i=1}^N w_{t,i} R_{t+1} = 1 -w_t^T R_{t+1}$ . The Generalized Method of Moments (GMM) was used to approximate the SDF $M$ vector. The first moment condition equation can be expressed as following: . $ underset{M}{ text{min}} underset{t}{ sum} || M(I_{t}) R^e_{t+1}||^2 $ . Since the above problem is convex, we can estimate the SDF for BTC using the cvxpy framework. . #4 - Solve GMM problem n = R.shape[0] #define weights for the SDF w = cp.Variable(shape=(I.shape[1] ,1),nonneg=True) #define SDF M = I @ w #Define expression Sigma = M @ R.T #Construct the problem prob = cp.Problem(cp.Minimize( cp.norm(Sigma) ) # , [cp.geo_mean(M) &gt;= 1 , ] , [cp.sum(w) == 1 ,cp.sum(M) == 1 ] ) prob.solve(verbose=True #, max_iters = 300 ) print(&#39;SDF weights from BTC: n&#39;,dict(zip(factors,w.value))) . def sharpe(x): return (x.mean() / x.std() * np.sqrt(365)) def plot_return(data,title): datac = data.copy() fig , ax = plt.subplots(figsize=(13,5)) y_return = (datac[y_label].pct_change().fillna(0)+1) p_return = ((datac[factors].pct_change().fillna(0) + 1) .apply(lambda x: (x @ w.value)[0] ,axis =1) .rename(&#39;SDF Portfolio&#39;) ) p_return_c = p_return.cumprod() y_return_c = y_return.cumprod() portfolio_benchmark = pd.concat([y_return_c,p_return_c],axis=1) portfolio_benchmark.plot(ax=ax) ax.legend(loc=0) ax.set(title=title,ylabel=&#39;growth factor&#39;,xlabel=&#39;date&#39;) return portfolio_benchmark def calc_metrics(data): #y_return = data[y_label].pct_change().fillna(0) p_return = ((data[factors].pct_change().fillna(0)) .apply(lambda x: (x @ w.value)[0] ,axis =1) .rename(&#39;SDF Portfolio&#39;) ) y_return = data.pct_change().fillna(0) y_return[&#39;SDF Portfolio&#39;] = p_return #portfolio_ret = pd.concat([y_return,p_return],axis=1) return &#39; nSharpe ration: &#39;+str(portfolio_ret.apply(sharpe).round(3).to_dict()) . Results . Below is the result of computing the SDF portfolio on the training dataset and plotting it against the index portfolio (BTCUSDT): . portfolio_benchmark = plot_return(df_train,&#39;In-sample: Performance of the SDF Portfolio&#39;) print(&#39;Portfolio growth (x times): n&#39;+ str((portfolio_benchmark.iloc[-1] - 1).round(2).to_dict())+ calc_metrics(df_train)) . Portfolio growth (x times): {&#39;BTCUSDT&#39;: -0.34, &#39;SDF Portfolio&#39;: 2.92} Sharpe ration: {&#39;BTCUSDT&#39;: 0.222, &#39;SDF Portfolio&#39;: 0.997} . Resulted plot and metrics for the hold-out dataset: . portfolio_benchmark_ho = plot_return(df_test,&#39;Out-of-sample: Performance of the SDF Portfolio&#39;) print(&#39;Portfolio growth factor: n&#39;+ str((portfolio_benchmark_ho.iloc[-1] - 1).round(2).to_dict())+ calc_metrics(df_test)) . Portfolio growth factor: {&#39;BTCUSDT&#39;: 2.84, &#39;SDF Portfolio&#39;: 11.08} Sharpe ration: {&#39;BTCUSDT&#39;: 2.704, &#39;SDF Portfolio&#39;: 3.394} . Conclusion . In this exercise, we estimated the SDF model for BTCUSDT using a GMM method and calculated the sample portfolio performance on the in-sample and hold-out time intervals. . The results of the model can be interpreted as follow: . Through the entire year of 2018, the BTCUSDT showed poor performance (-34%), resulting in the next-to-zero Sharpe ratio. On the same interval model portfolio showed a 300% growth but not without high volatility (Sharpe is still less than 1). . On the hold-out dataset both index and the model portfolio showed significant growth by 284% and 1108%, respectively. The Sharpe ratio for both index and the model portfolio is higher than 2: 2.7 and 3.4, respectively. . Given the results from the model run, we can confidently conclude that our SDF model portfolio achieved better performance than the index with preserving the higher Sharpe ratio. .",
            "url": "https://yourdatablog.com/cryptosdf/",
            "relUrl": "/cryptosdf/",
            "date": " • Dec 16, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "MMA",
            "content": "Table of Contents . 1&nbsp;&nbsp;Problem 1. Does equation $x^{2}+y^{2}+z^{2}=2 x y z$ have any non-zero integer solutions?1.1&nbsp;&nbsp;Problem 1 Solution 11.1.1&nbsp;&nbsp;Insight 1: consider mod 2 | . | 1.2&nbsp;&nbsp;Problem 1 Solution 2: Solution Trees1.2.1&nbsp;&nbsp;Insight 1: Vieta&#39;s Formula yields solution trees | 1.2.2&nbsp;&nbsp;Insight 2: Reduce the benchmark. | 1.2.3&nbsp;&nbsp;$x=y$ case | 1.2.4&nbsp;&nbsp;$z=1$ case | . | . | 2&nbsp;&nbsp;Problem 6. The kingdom of Mathlandia contains 2016 airports. Each airport is connected by a direct flight with at least 100 others. It&#39;s known that you can fly from each airport to any other, perhaps, with stopovers. Prove that, in fact, you can fly from any airport to any other making fewer than 65 connections.2.1&nbsp;&nbsp;Problem 6 Solution 1: Direct approach2.1.1&nbsp;&nbsp;Insight 1: do by induction | 2.1.2&nbsp;&nbsp;Since it is $2021$, what if this question was asked with $2021$ airports above would you need 57 connections? | . | . | . Problem 1. Does equation $x^{2}+y^{2}+z^{2}=2 x y z$ have any non-zero integer solutions? . Problem 1 Solution 1 . Insight 1: consider mod 2 . Right hand side is even so there are two cases x,y,z are all even or one of them is even. . Case 1: If one of them is even then LHS is 2 (mod 4) and RHS is 0 (mod 4). Contradiction. . Case 2:If all of them are even divide the equation by the highest power of two that divides all of them, i.e. $$x=2^k x_0,y=2^k y_0,z=2^k z_0$$ and at least one of $x_0,y_0,z_0$ is odd, but then dividing equation by $2^{2k}$ we get $$x_0^{2}+y_0^{2}+z_0^{2}=2^{k+1} x y z $$ and you can apply the argument from case 1 obtaining contradiction. . . Note: when writting a proof you can skip extra circle of logic and go straight to case 2, hence the proof would just be: . Let $k$ be highest power of 2 dividing $x$,$y$ and $z$, i.e $x=2^k x_0,y=2^k y_0,z=2^k z_0$ and at least one of $x_0,y_0,z_0$ is odd. Dividing equation by $2^{2k}$ we obtain $x_0^{2}+y_0^{2}+z_0^{2}=2^{k+1} x y z $ and since at least one of $x_0,y_0,z_0$ is odd, and RHS is even exactly two of $x_0,y_0,z_0$ must be odd but then LHS is 2 mod 4 and RHS is 0 mod 4. Contradiction. . Problem 1 Solution 2: Solution Trees . The point of this solution is to show how to structure solution for a much larger class of equations like this. . $$x^2+y^2+z^2=1xyz+2xy+3xz+4yz+5x+6y+7z+8$$ . where instead of $1,2,3,4,5,6,7,8$ you can have arbitrary integer numbers (that satisfy some mild conditions) and of course this can be generalized to more variables where LHS is sum of squares and right hand side sum of square-less terms i.e. polynomial in these variables without terms that are divisible by the square of any variable. . Which is generalization of Markov Diophantine Equation . $$x^2+y^2+z^2=3xyz$$ . See https://en.wikipedia.org/wiki/Markov_number for additional information. . Insight 1: Vieta&#39;s Formula yields solution trees . begin{align} 0 &amp;= x^{2}-(2 y z) x +y^{2}+z^{2}=(x-x_1)(x-x_2) x_1+x_2&amp; = 2yz x_1x_2&amp; = y^2+z^2 end{align}Hence if you have solution $(x_1,y,z)$ you can obtain $(x_2,y,z)$. . Insight 2: Reduce the benchmark. . There are several benchmarks we can take benchmark b(x,y,z)=max(|x|,|y|,|z|) or b(x,y,z)=|x|+|y|+|z| or b(x,y,z)=min(|x|,|y|,|z|). Let us not choose which benchmark we will use just yet and select it when it is the right time. . It also follows from first insight that $x_i$ are of the same sign (i.e. either both positive or both negative) and hence we can skip the signs and absolute values below focusing only on positive solutions (x,y,z). . So the plan is to show that via solution tree we can always get to a solution with benchmark less than some small number and check those by hand. . begin{align} x_1+x_2 &amp; = 2yz x_1x_2 &amp; = y^2 + z^2 end{align}which gives us begin{align} x_2 &amp; = 2yz - x_1 x_2 &amp; = frac{y^{2}+z^{2}}{x_1} end{align} . Let us deal with case when $x=y$ later and assume without loss of generality that $z &lt;y &lt; x_1, x_2 leq x_1$ and our goal is to show that $x_2 &lt; y$ that reduces benchmark $b=max(x,y,z)$ . So we can try something like this begin{align} x_2 &amp; = frac{y^{2}+z^{2}}{x_1} &lt; frac{2y^{2}}{x_1} &lt; frac{2y}{x_1} y &lt; frac 2z y end{align} where the last inequality comes from bound on $y$ or more precisely $ frac{2y}{x_1}$, which comes from from $x_1 + x_2 = 2yz$ that yiels $ frac 1z = frac {2y}{x_1+x_2} geq frac{2y}{2x_1} = frac {y}{x_1}$. . Hence we are done in case $z&gt;1$. . In fact we just showed that all solutions must either have two of $x,y,z$ equal or $z=1$, as otherwise we can keep descending benchmark ad infinitum. The rest is just dealing with these two cases: . $x=y$ case . The equation becomes $2x^2+z^2=2x^2z$ which $z^2=(2z-1)x^2$ which we can show via various methods including looking at mod 2 which brings us back to solution 1. . The tricky part here is that in most of that solutin of generalized Markov Diophantine Equation where $xyz$ term has coefficient 2 is the most tricky one, where the number of trees might be infinite. For case where this coefficient is greater than 2 , the solutions are rather well structured. . $z=1$ case . The equation becomes $0=x^2+y^2+1-2xy=(x-y)^2+1$ which obviously impossible as RHS is greater than 1, since squares are positive. . Problem 6. The kingdom of Mathlandia contains 2016 airports. Each airport is connected by a direct flight with at least 100 others. It&#39;s known that you can fly from each airport to any other, perhaps, with stopovers. Prove that, in fact, you can fly from any airport to any other making fewer than 65 connections. . Problem 6 Solution 1: Direct approach . Insight 1: do by induction . Let us consider airport $a$ and let $A_i$ be airports that are connected to it by exactly $i$ connections, i.e. you can reach it via $i$ connections but cannot reach it via $i-1$ connections. . Note: Special case $A_0$ just consists of a for the consistency of below calculations. Since all airports are connected it is enough to prove that $A_{65}=0$ as this would imply that all airports are within $A_0,A_1,A_2...A_{65}$. In fact we will show $A_{58}=0$ . Note that airports that for $i&lt;j$, $a_1 in A_i$ and $a_2 in A_j$ can only be connected if $j-i leq 1$; as otherwise you can reach $a_2$ via going to $a_1$ in n flights and then flying to $a_1$ which would be $i+1$ connections which is less than $j$ which implies $a_2 notin A_j$. . From this we get that each airport in $A_i$ can only be connected to airports in $A_{i-1}, A_{i}$ or $A_{i-1}$. Hence since each airport has at least $100$ connections from each airport we get $A_i&gt;0$ implies $|A_{i-1}|+|A_{i}|+|A_{i-1}| geq 101$. . If $|A_{57}| neq 0$ hence $A_{3k}+A_{3k+1}+A_{3k+2} geq 101$ for $k=0,1,...19$ and hence $$|A_0+A_1+...+A_{59}|=|(A_0+A_1+A_2)+(A_4+A_5+A_6)+...(A_{57}+A_{58}+A_{59})| geq 20 cdot 101=2020$$ which contradicts the fact that there are only 2016 towns. . Note: we can start this sum from $|A_0+A_1| geq 101$ prove in same way $|A_{57}|=0$ . Since it is $2021$, what if this question was asked with $2021$ airports above would you need 57 connections? . Indeed: here is a counterexample and in fact you need 58. . [1, 100, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 1, 99, 1, 100, 1] length: 59 sum: 2021 minimum sum: 101.0 . In fact this is an optimal counterexample for these problems starting from $1,100$ then a sequence of $1,1,99$ possibly truncated in the last triple, and then finishing with 100,1. . More generaly in a $ it{connected}$ $ it{graph}$ $G$ if each $vertex$&#39;s $valency$ $(degree)$ is at least $V$, then the number of vertecies that are distance $D$ from any vertex is at least $min(|G|,f(D,V))$, where: . begin{align} f(0,V)&amp;=1=1 f(1,V)&amp;=1+V&amp;=V+1 f(2,V)&amp;=1+V+1&amp;=V+2 f(3,V)&amp;=1+V+V+1&amp;=2V+2 f(4,V)&amp;=1+V+1+V+1&amp;=2V+3 f(5,V)&amp;=1+V+1+1+V+1&amp;=2V+4 f(6,V)&amp;=1+V+1+1+(V-1)+V+1&amp;=3V+3 f(7,V)&amp;=1+V+1+1+(V-1)+1+V+1&amp;=3V+4 f(8,V)&amp;=1+V+1+1+(V-1)+1+1+V+1&amp;=3V+5 f(3k-3,V) &amp;=&amp; k cdot (V + 1) f(3k-2,V) &amp;=&amp; k cdot (V + 1) + 1 f(3k-1,V) &amp;=&amp; k cdot (V + 1) + 2 cdots end{align}where the term in the middle represents counterexample which can be proven optimal by the fact that $|A_i|&gt;0$ implies $|A_{i-1}+A_{i}+A_{i+1}| geq V+1$ and that $A_i$ in the middle are at least of size 1. .",
            "url": "https://yourdatablog.com/mma/",
            "relUrl": "/mma/",
            "date": " • Jul 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Optimizing Risks for a Portfolio of Cryptocurrencies",
            "content": "In this post we will take a task of optimizing a portfolio of cryptocurrency and apply treasury risk optimization algorithms there. Let&#39;s start with basics. What is a typical task of a treasury manager (if we focus on risk optimization). Well, you&#39;ve got a portfolio of positions (things you buy or sell) and currencies. You would like to protect your company from volatility and optimize financial performance. One of the algorithms successfully used there is a Stochastic Discount Factor calculation . TBD: add link, that allows us, etc. . This approach has been successfully applied to stock options trading and, presumably, can be applied to FICC trading also - pursuing the risk optimization. Now, for it to work we need prices for future contracts of different durations for number of years, per instrument. it is not easy for a common person to get such data for free (because holders of such data are rather greedy). Also, a casual reader won&#39;t related to oil future contracts. So we have created an imaginary enterprise - someone is mining ethereum and bitcoin (those are your goods) and paying electricity for mining stations in US Dollars (costs in functional currency). . This guy wants to optimize his enterprise performance by selling future contracts so that his risk exposure is fixed (e.g. 1 BTC) . Copyright &copy; Maxim Korotkov; 2021. This article is published under a Creative Commons Attribution 4.0 International License. .",
            "url": "https://yourdatablog.com/cryptosdf_draft/",
            "relUrl": "/cryptosdf_draft/",
            "date": " • Apr 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "The Rainbow Method Part I. Encoding Categorical Variables.",
            "content": ". Introduction . &quot;I have 2000 features and I need to get to 50 features and keep model as good or better&quot; was how this all started. This is familiar to anyone in institutional data science where you need to carefully vet every model and look at each feature to make sure it is fine from the standpoint of regulation. What is valuable about this story is that it is based on a real production model that was developed by Anna Arakelyan at MassMutual and a method suggested by Dmytro Karabash. We will get to detailed results at the end but here&#39;s a preview results for anyone who want to take a peak: . The rainbow method outperforms the One-hot easily. In fact, the rainbow method even shows better result for 50 features than One-hot does for 100. Also note that when droping from 50 to 10 features, the reduction in macro-f1 if you use One-hot is 6 times that of the rainbow method (3 times for kappa and accuracy, 2 times for macro-auc). . Both models actually keep improving as the number of features goes up. Though the improvement is substantially faster and larger for the rainbow method. . In this series of articles we will introduce the Rainbow encoding method and will discuss its advantages over the conventional One-hot. Part I below explains how the method works, shows that it preserves full data siganl, and provides examples of its use for various kinds of categorical variables. Part II explores its empirical application to a real business data science problem - Mindset Segmentation model at MassMutual using Acxiom database. Part III provides mathematical foundation for the method and shows that it reduces model complexity and helps avoid overfit. . Background . Data encoding is a crucial part of any data science pipeline. No matter which machine learning algorithm is used - there is no avoiding of data cleaning and encoding. Especially with dirty and complex real data sets, ensuring the most appropriate and efficient way of feature encoding and engineering is a nontrivial problem. While encoding quantitative and binary columns is usually a straightforward task, the encoding of categorical variables deserves a deeper look. One of the most popular encoding methods for categorical variables has been the One-hot procedure. It generates an indicator variable for every category and, thus, creates a set of K new features, where K is the number of categories. One significant implication of this method is the dimensionality increase. Depending on the magnitude of K, it can have various undesirable technical consequences: substantial raise of computational complexity, loss of degrees of freedom (N-K, where N is the number of samples), multicollinearity, and, ultimately, an overfitted model. Non-technical consequences include unnecessary complication of the model that should be communicated to final users and that contradict the law of parsimony aka Occam&#39;s razor. . Method . We call it rainbow from a very simple analogy: if you have a categorical column with value names &quot;Red&quot;, &quot;Orange&quot;, &quot;Yellow&quot;, &quot;Green&quot;, &quot;Blue&quot;, &quot;Indigo&quot; and &quot;Violet&quot;, in other words - colors of the rainbow, instead of One-hot encoding, you can simply create one feature with encoding: . . 0 = &quot;Red&quot; 1 = &quot;Orange&quot; 2 = &quot;Yellow&quot; 3 = &quot;Green&quot; 4 = &quot;Blue&quot; 5 = &quot;Indigo&quot; 6 = &quot;Violet&quot; . This nicely replaces seven One-hot features with one rainbow. . Most of the nominal variables, from the first glance, seem like they cannot be converted to a quantitative scale. This is where we suggest finding a rainbow. The seemingly unordered categories acquire a perfect order if we find a scale, where each category fits into its own unique place. With the color - the natural scale is Hue. But it is not the only one. We can think of, say, brightness, saturation, color temperature, etc. We invite you to experiment with a few different rainbows that might capture different nuances of the categorical quality. . You can actually make and use two or more rainbows out of one categorical variable depending on the number of categories K and the context. . We don&#39;t recommend to use more than $ log_2 {K}$ rainbows because we do not want to surpass a number of encodings in a binary One-hot. . Why is rainbow better than any other choice? It is very simple and intuitively sensible. In many cases it is not even that important which rainbow you choose (and by that we mean the color order), it would still be better than using One-hot. The more natural orders are just likely to be slightly better than others and easier to interpret. . Hence our motto:&quot;When a nature gives you a rainbow, take it...&quot; . Important note: this method is highly efficient in conjunction with the models that rely on variables as ranks rather than exact values. For example, decision trees, random forest, gradient boosting - these algorithms will output the same result if, say, the variable Number of Children is coded as . . 0 = &quot;0 Children&quot; 1 = &quot;1 Child&quot; 2 = &quot;2 Children&quot; 3 = &quot;3 Children&quot; 4 = &quot;4 or more Children&quot; . . or as . . 1 = &quot;0 Children&quot; 2 = &quot;1 Child&quot; 3 = &quot;2 Children&quot; 4 = &quot;3 Children&quot; 5 = &quot;4 or more Children&quot; . . or even as . . -100 = &quot;0 Children&quot; -85 = &quot;1 Child&quot; 0 = &quot;2 Children&quot; 10 = &quot;3 Children&quot; 44 = &quot;4 or more Children&quot; . . as long as the correct order of the categories is preserved. The values themselves don&#39;t carry the quantitative function in these algorithms. It is the rank of the variable that matters, and a tree-based algorithm will use its magic to make the most appropriate splits to introduce new tree nodes. . The application of our method for other algorithms such as Linear Regression, Logistic Regression is out of scope of this article. We expect that such way of feature engineering would be still beneficial, but that is a subject of a different investigation. . Note that by using a rainbow rather than One-hot we are not losing any of the data signal. Let&#39;s explore that question below. . Rainbow keeps the full signal . Let&#39;s zoom in for a minute on a classical rainbow example with just 4 values: . . Color 0 = &quot;Red&quot; 1 = &quot;Yellow&quot; 2 = &quot;Green&quot; 3 = &quot;Blue&quot; . . In the case of One-hot, we would create 4 features: . Color_Red = 1 if Color = 0 and 0 otherwise, Color_Yellow = 1 if Color = 1 and 0 otherwise, Color_Green = 1 if Color = 2 and 0 otherwise, Color_Blue = 1 if Color = 3 and 0 otherwise. . In the case of Rainbow, we would just use the Color itself. . Let&#39;s compare the possible models made using these two methods: 4 features vs 1 feature. Let&#39;s for simplicity build a single decision tree. Let&#39;s look at few scenarios of the data generating process. . Scenario 1 . Assume all the categories are wildly different, and each one introduces a substantial gain to the model. That means each One-hot feature is indeed critical - the model should separate between all 4 groups created by One-hot. . In that case, an algorithm such as xgboost will just make the splits between all the values which is perfectly equivalent to One-hot. So the same exact result is achived with jsut one features instead of four. . Figure 1 . . One can clearly see that this example is easily genearlized to One-hot with any number of categories. Also note that for such data generating process the order of the categories in Rainbow does not even matter as splits will be made between all categories. In practice, (K-1) splits will be sufficient for both methods to separate between K categories. . Figure 2 . . The main takeaway is that not a bit of data signal is lost if one switches from One-hot to Rainbow. In addition to this, depending on number of categories, a substantial dimensionality reduction happens which is beneficial for multiple reasons. . Scenario 2 . If the chosen rainbow (Figure 2) is not the best because, for example, data generating process separates between the group of {Red, Green} and {Yellow, Blue}, then once again - the algorithm will make all the necessary splits. . Figure 3 . . Here we show that depending on the order of binary features picked by One-hot there can be different trees built. In case of Rainbow, though - once again we demonstrate that no information is lost and a (K-1) splits tree is built even in this least favorable scenario. . Scenario 3 . Finally, if the data generating process is actually in agreement with the Rainbow order, then Rainbow method would be superior to One-hot. Not only will it not lose any data signal, it will also subtantially reduce complexity, decrease dimensionality, and help avoid overfit. . Suppose, the true model pattern only separates between {Red, Yellow} and {Green, Blue}. In that case, Rainbow has a clear advantage as it exploits these groupings while One-hot does not. . Figure 4 . . In this introductory article, we briefly show that by substituting One-hot with Rainbow, we save dimensionality without any informational loss. In Part III though we dive deeper in the mathematical foundation of the Rainbow method and actually prove that Rainbow method outperforms One-hot in the worst, average, and best scenarios. . In short, if the rainbow order is even somewhat meaningful, the algorithm would capture the similarity in the adjacent values by making a smart decision about the number and location of splits. Thus, we will receive a simpler and better model. . Examples of Categorical Variables . The application of our method depends highly on the level of measurement of the treated variables. While quantitative variables have a ratio scale, i.e. they have a meaningful 0, ordered values, and equal distances between values; categorical variables usually have either interval, or ordinal, or nominal scales. Let us illustrate our method for each of these types of categorical variables. . Interval variables have ordered values, equal distances between values, but the values themselves are not necessarily meaningful, for example, 0 does not indicate an absence of a quality. The common examples of interval variables are Likert scales: . . How likely is the person to buy a smartphone mobile phone? 1 = &quot;Very Unlikely&quot; 2 = &quot;Somewhat Unlikely&quot; 3 = &quot;Neither Likely, Nor Unlikely&quot; 4 = &quot;Somewhat Likely&quot; 5 = &quot;Very Likely&quot; . . In a straightforward way, if we simply use the raw values 1 through 5, that will be the best and most natural rainbow. The algorithm such as xgboost will make the appropriate splits instead of being forced to use the splits predetermined by One-hot, which in many cases is simply an overfit. . Ordinal variables have ordered values that are meaningless, and the distances between values are also not equal or not even explainable. An example: . . What is the highest level of Education completed by the person? 1 = &quot;No High School&quot; 2 = &quot;High School&quot; 3 = &quot;Associate Degree&quot; 4 = &quot;Bachelor&#39;s Degree&quot; 5 = &quot;Master&#39;s Degree&quot; 6 = &quot;Doctoral Degree&quot; . . Similar to interval variables, raw ordinal numeric codes can be used in the model without introducing any issues, as long as the order is correct. In some cases, a variable is intrinsically ordinal, but the given numeric codes don&#39;t follow the correct order - in these situations we could simply reorder categories and then use this updated variable as a quantitative feature. . The use of natural rainbow for interval and ordinal variables is likely not the cause for concern because it is clearly a perfect alternative to One-hot for tree-based algorithms. The more complicated and non-obvious question is how to treat nominal variables. . Nominal variables have no obvious order between categories, and are almost always encoded with One-hot method. This is where finding a rainbow becomes valuable. It will turn something, that seems completely non quantitative, into a quantitative scale. In our classical example, a nominal variable Color . . A = &quot;Red&quot; B = &quot;Blue&quot; C = &quot;Green&quot; D = &quot;Yellow&quot; . . can be replaced by the newly engineered rainbow feature . . 1 = &quot;Blue&quot; 2 = &quot;Green&quot; 3 = &quot;Yellow&quot; 4 = &quot;Red&quot; . . Lets us show a few other examples of creating rainbows. . . Vehicle Type C = &quot;Compact Car&quot; F = &quot;Full-size Car&quot; L = &quot;Luxury Car&quot; M = &quot;Mid-Size Car&quot; P = &quot;Pickup Truck&quot; S = &quot;Sports Car&quot; U = &quot;SUV&quot; V = &quot;Van&quot; . . For this variable we can think of dozens of characteristics to make a rainbow - vehicle size, capacity, price category, average speed, fuel economy, costs of ownership, motor features, etc. Which one (or more than one) to pick? The choice depends on the context of the model and what are you trying to predict. The preference should be given to the ones that provide more explanatory power and/or make sense from the interpretation standpoint. . Consider another variable: . . Marital Status A = &quot;Married&quot; B = &quot;Single&quot; C = &quot;Inferred Married&quot; D = &quot;Inferred Single&quot; . . If we think about Single and Married as the two ends of the spectrum, then Inferred Single could be between the two ends, closer to Single, while Inferred Married would be between the two ends, closer to Married. That would make sense because Inferred holds certain degree of uncertainty. Thus, the following order would be reasonable: . . 1 = &quot;Single&quot; 2 = &quot;Inferred Single&quot; 3 = &quot;Inferred Married&quot; 4 = &quot;Married&quot; . . In case there are any missing values here, the Unknown category fits exactly in the middle between Single and Married as there is no reason to prefer one end over the other. So the modified scale could look like this: . . 1 = &quot;Single&quot; 2 = &quot;Inferred Single&quot; 3 = &quot;Unknown&quot; 3 = &quot;Inferred Married&quot; 4 = &quot;Married&quot; . . Another example: . . Occupation 1 = &quot;Professional/Technical&quot; 2 = &quot;Administration/Managerial&quot; 3 = &quot;Sales/Service&quot; 4 = &quot;Clerical/White Collar&quot; 5 = &quot;Craftsman/Blue Collar&quot; 6 = &quot;Student&quot; 7 = &quot;Homemaker&quot; 8 = &quot;Retired&quot; 9 = &quot;Farmer&quot; A = &quot;Military&quot; B = &quot;Religious&quot; C = &quot;Self Employed&quot; D = &quot;Other&quot; . . Finding a rainbow here might be harder, but here are a few ways to do it: we could order occupations by average annual salary, by its prevalence in the geographic area of interest, or by information from some other dataset. That might involve calling a Census API or some other data source, and might be complicated by the fact that these values are not static, but these are still viable solutions. . What if there is no natural rainbow? . In some situations though we cannot find a logical order for the rainbow because the variable itself is not interpretable. An example could be a black box column made by a third party: . . Financial Cluster of the Household 1 = &quot;Market Watchers&quot; 2 = &quot;Conservative Wealth&quot; 3 = &quot;Specific Savers&quot; 4 = &quot;Tried and True&quot; 5 = &quot;Trendy Inclinations&quot; 6 = &quot;Current Consumers&quot; 7 = &quot;Rural Trust&quot; 8 = &quot;City Spotlight&quot; 9 = &quot;Career Conscious&quot; 10 = &quot;Digital Financiers&quot; 11 = &quot;Financial Futures&quot; 12 = &quot;Stable Influentials&quot; 13 = &quot;Conservatively Rural&quot; . . In this case we might not have a clear idea how to order categories due to lack of knowledge of what each category entails. What to do with such variables? . We recommend creating an artificial rainbow by looking at how each category is related to the target variable. . The simplest solution is to place categories in the order of correlation with target variable. So the category with the highest value of correlation with the dependent variable would acquire numeric code 1, and the category with the lowest correlation would acquire numeric code 13. So our rainbow in this case would mean the relationship between the cluster and the target variable. This method would work for both - classification and regression models - as it can be applied to a discrete and to a continuous target variable. . Alternatively, you can construct your own rainbows by merely utilizing certain statistical qualities of the categorical variable and the target variable. . For instance, in case of a binary target variable, we could look at the proportion of ones given each of the categories. Suppose, among Market Watchers percent of positive targets is 0.67, while for Conservative Wealth it is 0.45. In that case, Market Watchers will be ordered higher than Conservative Wealth (or lower, if the target percent scale is ascending). In other words, this rainbow would reflect the prevalence of positive target inside each category. . In case of a multiclass classification, we could create rainbows for each class to represent relationship between categories and each class separately. . In Part II we will demonstrate that the Rainbow method outperforms One-hot using empirical example of a real data science project at MassMutual. . The Rainbow Method Part II. Application to a Real Data Science Problem . We will illustrate the effectiveness of the rainbow encoding method using the real data science project developed in the Customer Journey domain of Data Science at MassMutual - a life insurance company with over 170 years of history which takes pride in growing a large team of data scientists, engineers, and technologists to support most of the business decisions. . Business Use Case . In a nutshell, the data task is a multiclass classification problem that aims to predict one of the five Mindset Segments for each of the prospective customers. . . The segmentation framework represents five classes that reflect a person&#39;s age, financial stability, and attitude towards financial decisions. The predicted segments are then used by MassMutual marketing team in different types of campaigns for targeting and customization. For example, Self-Assured customers would value more independence and autonomy in making decision of buying a life insurance policy whereas Day to Day customers would value having a guidance and thorough explanations of different financial products by a dedicated advisor. . We have a small set of labeled indivuduals (17.5K persons). The labels are provided by the MassMutual vendor which ran Mindset surveys and designed segment assignment rules. For simplicity, let&#39;s assume we know nothing about these rules and take labels as given. . Then we attach the columns from our main prospect database to these 17.5K persons. The goal is to learn the best model using these target labels and the available features, and then predict segments for all other (unlabelled) prospective customers. . The main database for this problem is provided by Acxiom and covers about 300 columns representing rich set of demographic characteristics, composition of the household, income and net worth, financial behavior, and digital savvyness. . Using Acxiom data and the Mindset Segmentation problem, we will compare the conventional One-hot encoding with the Rainbow method. For this 5-class classification task, we will demonstrate a few standard metrics - Accuracy, Cohen&#39;s Kappa, Macro Avg F1 score, and Macro Avg AUC ROC. . Cohen&#39;s Kappa, F1 and AUC ROC are very helpful for an unbalanced multiclass classification problem. Accuracy is not the best metric for such task, and is used merely for interpretation and comparison purposes. . Categorical Variables . We took all the existing categorical variables in Acxiom database - interval, ordinal, and nominal, and excluded quantitative and binary variables. The idea was to show the pure difference in the model performance between the two types of encoding for the same set of catogorical factors. . We then applied a target stratified 4-fold Cross Validation split. All the data processing from this point on is done inside the cross validation loop, i.e. the creation of One-hot features and Rainbow features is learned from each fold train set and applied to each fold validation set. . The total set of 111 variables was transformed into 201 Rainbow features and, alternatively, into 2260 One-hot features (with very slight deviations in N in 4 different folds). . Type of variable N raw N Rainbow encoded N One-hot encoded . Interval | 64 | 64 | 1670 | . Ordinal | 14 | 14 | 178 | . Nominal | 33 | 123 | 412 | . Total | 111 | 201 | 2260 | . While interval and ordinal variables have a straightforward rainbow tranformation - one can notice that 64 interval features turned into 64 rainbows and 14 ordinal features turned into 14 rainbows, the nominal variables tranformation was a bit more involved. Out of 33 nominal variables, for 23 we found a natural rainbow, while for 10 remaining variables we applied correlation ordering and target percent ordering (see Part I). Since we deal with 5 classes, we made 10 new features for each of these variables. . For example, given the name of feature &quot;Financial_Cluster&quot; and 5 segment names, we made features . Financial_Cluster_Self-Assured_correlation_rank | Financial_Cluster_Juggler_correlation_rank | Financial_Cluster_Starter_correlation_rank | Financial_Cluster_Day to Day_correlation_rank | Financial_Cluster_Well-Establishes_correlation_rank and | Financial_Cluster_Self-Assured_target_percent | Financial_Cluster_Juggler_target_percent | Financial_Cluster_Starter_target_percent | Financial_Cluster_Day to Day_target_percent | Financial_Cluster_Well-Establishes_target_percent | . In this way, 33 raw nominal variables turned into 123 Rainbows. . It is important to note that the search for natural or non-natural Rainbows is highly project and context specific, and is more of an art than a craft. For instance, for a binary classification problem, there probably would have been only one or two Rainbows. . We ran all XGBoost models covering this hyperparameter space: . &#39;objective&#39;: &#39;multi:softprob&#39; &#39;eval_metric&#39;: &#39;mlogloss&#39; &#39;num_class&#39;: 5 &#39;subsample&#39;: 0.8 &#39;max_depth&#39;: [2, 3, 5] &#39;eta&#39;: [0.1, 0.3, 0.5] &#39;n_estimators&#39;: [50, 100, 200] . All the results below in this article represent cross validation average metrics. . Aggregate Results . Let us start with overall averages for all runs. Clearly, the average metrics across all models are notably higher for Rainbow encoding. . Hyperparameters . The following plots show metric dynamics depending on every hyperparameter keeping other hyperparameters constant. These plots also clearly demonstrate the superiority of Rainbow method for every hyperparameter and every metric. . Runtime . Next, let&#39;s compare the runtime for each method. . One-hot: 66.074 s Rainbow: 7.444 s . Average time to run a single rainbow model is 9 times faster that of the single One-hot model! So, in addition to a substantial increase in model performance metrics, Rainbow method can also save data scientists huge amount of time. . Interval, Ordinal, and Nominal . Next, we ran the models that covered the bundles of interval, ordinal, and nominal features separately. Below are the results. . These results demonstrate again that Rainbow is preferred to One-hot. As expected, interval and ordinal features gain the most from Rainbow encoding, while nominal variables - less so. . Feature Selection . Finally, to make a comparison more fair in terms of dimensionality, we picked top 10, top 50, and top 100 features from each feature set - Rainbow and One-hot. We used feature importance attribute of the XGBoost model and aggregated feature importance scores for 4 cross validation folds on the best hyperparameter set for each encoding type. Below are the results. . Every single metric is considerably higher for the Rainbow method, especially with the lower number of selected features. As mentioned in Part I, rainbow encoding with 50 features is even better than One-hot with 100 features as shown by macro-f1, kappa, and accuracy. Also, note the drop from 50 to 10 features in One-hot versus rainbow - macro-f1 looses 0.065 points with One-hot compared to only 0.016 points with Rainbow. . Conclusion . As we illustrated above, the Rainbow method is an elegant and simple way to encode categorical variables, that will significantly reduce data dimensionality without losing any part of valuable signal; that will likely cause substantial improvements in model performance metrics (or, at the very least, will not cause any reductions in metrics); and that will save great amount of time for modelers. . Finally, we should note that this article just opens the conversation about the Rainbow method, and by no means exhausts the topic. In the potential future investigations, we could explore some other aspects. To name a few: binary and continuous target variables; comparison with other dimensionality reduction methods, such as PCA; missing values and how they fit the Rainbow framework; limitations and implications of combining seemingly unrelated features into a single Rainbow. We hope to open the gate for further questions and feedback on this method. . The Rainbow Method Part III. Mathematical Foundation. . In this article we justify mathematically the advantages of Rainbow method over One-hot. We start with a simple example, and then we proceed with a full generalization and a simulation. In the final part we discuss various assumptions and limitations of the Rainbow method and provide actionable recommendations to get the best out of Rainbow. Please note that the discussion below applies to any tree-based algorithm that relies only on order of the values. That includes any random forest and any gradient boosting algorithm such as xgboost. . How exactly does rainbow help? . Simple Example . Consider the following example: . assume a binary classification problem, with output variable having values 0 and 1; | assume we have only one categorical variable V and no other variables; | assume V has exactly four categories with values v1 &lt; v2 &lt; v3 &lt; v4; In case of the nominal variable the values simply represent numeric codes assigned to each category. | assume each value of V has same or close number of samples in the data; | assume an ideal separation will give to {v1 &amp; v2} samples f-score = 1 and to {v3 &amp; v4} samples f-score = 2, where f-score represents some decision function used for separation (e.g. Gini impurity, information gain, etc.). I.e. the data generating process would separate between the groups {v1 &amp; v2} and {v3 &amp; v4}, but would not distinguish values within each group. | . Let&#39;s consider two cases - Rainbow and One-hot. For Rainbow, we will use V directly to build a decision tree. For One-hot, we will need to create four binary variables - let&#39;s call them V1, V2, V3, V4, where Vi = 1 if V=vi and 0 otherwise. . Let&#39;s for simplicity build a single decision tree. Assume a simple algorithm that picks each next feature randomly, and compares all values of that feature to make the best split. In the case of rainbow, samples from two categories will be sent to the left branch and samples from two other categories will be sent to the right branch of the tree. In such a situation a natural split occurs between v2 and v3, and rainbow would need one split, . Figure 1 . . while One-hot would need two or three splits. Here are two examples: . Figure 2 . . Figure 3 . . In other words, assuming a clear order between categories, Rainbow method is strictly better at separating samples - it makes less splits. In this example, it finds the single perfect split between v2 and v3. One-Hot method goes through four scales instead of one and does not naturally incorporate combination of information from a few features. . Figure 4 . . But what happens if the categories do not go in a perfect order? . Generalizing simple example . Let&#39;s look at the more general case, where the values of the categorical variable are in different order. Below are some examples of 4 categories and their f-scores. Same f-score justifies combining values into a group. For instance 1,1,2,2 represents our example above, where first two categories get assigned f = 1, and last two categories get assigned f = 2, and the split should happen in the middle. . The number of splits would depend on the order in which these categories appear. . For rainbow orders: . 1,1,2,2 and 2,2,1,1 the number of splits would be 1; | . Figure 5 . . 1,2,2,1 and 2,1,1,2 the required number of splits would be 2; | . Figure 6 . . 1,2,1,2 and 2,1,2,1, the required number of splits would be 3. | . Figure 7 . . For One-hot, the orders below would represent the order in which the One-hot features are selected and added to the tree: . 1,1,2,2 and 2,2,1,1 the required number of splits would be 2; | . As in our simple example above (Figure 2), the first split would be made on V1, the second on V2, and that would be sufficient to separate between the samples with f = 1 and f = 2. . 1,2,2,1 and 2,1,1,2 and 1,2,1,2 and 2,1,2,1 the number of splits would be 3. | . In these examples, as shown above (Figure 3), three splits would be needed given the order in which binary features enter the tree. . Note that the Rainbow and One-hot orders described here are not the same orders, so we cannot compare them directly. In case of Rainbow, we look at the order at the top - in which categories appear on a single scale, while for One-hot we look at the order at the bottom - in which categories are picked by the model. Nevertheless, we see that on average Rainbow is better as it provides lower number of splits - best case 1, average case 2, worst case 3; while One-hot gives best case 2, average case $ frac 83$ and worst case 3. In fact, this is true in general and we show it below. . General Case: Random rainbow vs One-hot . Assume that a certain categorical variable $V$ has $K$ categories with values v1 &lt; v2 &lt; v3 ... &lt; vk. Then: . One-hot would create $K$ features $V1, V2, ..., VK$, where $Vi$ = 1 if $V$ = vi and 0 otherwise; | Rainbow would create a single feature, where all the categories appear on one scale. We can keep the name of this variable $V$. | . Assume $K$ categories optimally would need to be split into $g$ groups of size $G_1,G_2,...,G_g$. By size $G_i$ we mean number of categories out of $K$ (not the number of samples). . For example, in the simple example above - the order is 1,1,2,2: . Figure 8 . . and $K=4$, $g=2$, $G_1=2$ consisted of {v1, v2}, and $G_2=2$ consisted of {v3, v4}. . Another example - if the order is 1,2,2,1: . Figure 9 . . Here $K=4$, $g=2$, $G_1=2$ consists of {v1, v4}, and $G_2=2$ consists of {v2, v3}. . Unlike the simple example of the perfect rainbow, in the general case the $K$ categories would appear in random order, and our rainbow would be random. However for the purposes of computation we only care about the groups of same f-values rather than about actual categories. In other words, if v1 and v2 samples produce same f-score, it does not matter whether the order is (v1, v2) or (v2, v1). . For One-hot the order that would matter is the one in which One-hot splits are made. This order is also assumed to be random. This is the order that we would always refer to when talking about One-hot. . In reality the One-hot splits are not fully random and the order of them being picked by the model varies depending on the number of samples in each category, subsamples of columns chosen and other hyperparameters. That can be both in favor and against the case for One-Hot. We will come back to this assumption later in the article, and for now will proceed assuming randomness for simplicity. . While the orders that matter for Rainbow and One-hot are not the same, it helps to couple them to make comparisons. Let&#39;s compute the required number of splits in best, average and worst cases. . Main Comparison Metric . As in the simple example above, in this general case of random rainbow vs One-hot we will compare number of splits made by the model to approximate the data generating process. Why is it our main metric? Less splits means: . less time; | less computational resources; | simpler model; | less overfit; | more effective model. | . Best case . One-hot . In the best case scenario for One-hot, the biggest group won&#39;t be split. . For example, in the case of 1,1,2,2,2,3,3 we would like the biggest group 2,2,2 to end up on the bottom of the tree, and thus, not introduce any extra splits. . Figure 10 . . Thus, in best case One-hot would require $K - G_{max}$ splits, where $G_{max} = max_i G_i$ is the maximum of $G_i$. The best case $K - G_{max}$ is obtained when each group except the largest one gets one split per category and the remaining largest group has the same answer so doesn&#39;t require any additional splits. To prove that one cannot do better, observe that if a few categories do not appear on any tree node, i.e. no splits are made, then they are &quot;combined&quot; - there is no way to distinguish the values in these two categories. . Rainbow . In the best case of rainbow - the values are in perfect order, and the minimum splits is needed. Given $g$ groups, that would require $g-1$ splits. That happens when groups are together and it simply splits at $g-1$ points to separate $g$ groups. . For example, clearly two splits are needed for the perfect rainbow with three groups 1,1,2,2,2,3,3. . One cannot do better since if there is no split between two groups there is no way to distinguish them. . Comparison . Clearly $$K - G_{max}=( sum_{i=1}^g G_i)-G_{max}=( sum_{i!=max}G_i) geq ( sum_{i!=max} 1) = g-1$$ and hence best rainbow is better than best One-hot. . Average Case . One-hot . From the logic presented in best case, it follows that . The number of splits that One-hot needs would be $N$ - number of categories that are chosen before remaining ones are in one group. . Figure 10 shows the best case - where all the remaining ones belong to the largest group. In average case, the remaining group can be any group, or any group&#39;s part. We cannot expect that a homogenous group won&#39;t be broken into pieces when building an average tree. What we care about is that the tree building should end when reaching some homogenous group. . In the best case $N = K - G_{max}$. In the average case $N = K - R$ where $R$ is the size of the remaining group. . For example, if the order in which One-hot categories are chosen for the tree is 1,1,2,2,3,3,2,2,2 then $R=3$ since the last group 2,2,2 is of size 3. The number of categories $K=9$, thus the number of splits would be $K-R=6$. . It is easiest to compute expected value of $N$ by using less common formula $ mathbb E[X] = sum_{j} mathbb P[X geq j]$ applied to $R$. We then utilize the formula of hypergeometric distribution to calculate each probability and achieve the result below. . begin{align} mathbb E[N] &amp;= mathbb E[K-R] = K- mathbb E[R] = K - sum_{j=1}^{K} mathbb P[R geq j] =K- sum_{j=1}^K frac{ sum_i frac{G_i!}{(G_i-j)!}} { frac{K!}{(K-j)!}} = (K-1) - sum_{j=2}^K frac{ sum_i frac{G_i!}{(G_i-j)!}} { frac{K!}{(K-j)!}} end{align} Rainbow . From the best case logic it also follows that . The number of splits that random Rainbow needs would always be $D$ - number of consecutive pairs in rainbow order that are in distinct groups. . In the best case $D=g-1$. . For example, in the case of Rainbow order 1,1,2,2,3,3,2,2,2 the splits will happen in 3 places, and these are the only places where the group is changed and the values in consecutive pairs belong to distinct groups. . Let&#39;s calcualte expected value of $D$. We do this by calculating for $K-1$ consecutive pairs as potential places for split the probability that the f-score is different. It is easier to subtract from one the probabilities that f values in each consectutive pair are same. We sum through all possible groups $i$ to compute likelihood that two consecutive values belong to one group. . begin{align} mathbb E[D] &amp;= (K - 1) cdot (1- frac{ sum_i G_i cdot (G_i-1)} {K(K-1)}) notag &amp;= (K-1)- frac{ sum G_i cdot (G_i-1)} {K} notag end{align}Comparison . An easy way to see that $ mathbb E[D]$ is larger than $ mathbb E[N]$ is the stochastic coupling, since both $N$ and $D$ can be obtained as starting from $K-1$ and subtracting $1$ for each consequitive pair in the same group; except in case of $N$ all these pairs have to be in string at the end. Due to this restriction $N$ is always larger. . Worst Case . One-hot . In the worst case for One-hot the number of splits is $K-1$ when one goes through all but one category to get to one group remaining. . Rainbow . In this case the number of needed splits is $ min( (K - 1), 2 cdot (K-G_{max}))$ as we can only get up to two splits per each category not in the largest group. . Indeed, if there is no consecutive pairs of values that belong to the same group then the number of splits would be $K-1$, for example here 3,2,1,3,1,2. . At the same time, if there is a prevalent group and some minority groups, for example 1,1,1,2,1,1,3,1,1, the number of splits will equal to the number of minority groups - $K-G_{max}=2$ in this case (2 and 3) times 2 which is the maximum number of splits that each minority group introduces - one to the left and one to the right. . Thus the needed number of splits will be the minimum between the two cases above. . Comparison . Clearly the number of splits for Rainbow is less than or equal to that of One-hot for the worst case as well. . Simulation . We have shown above that in a general case Rainbow method outperforms One-hot in best, average, and worst case scenarios. Let&#39;s look at the empirical simulation of number of splits for various kinds of data generating process. . rainbow_vs_onehot.T.plot(rot=45,figsize=(12,8)) plt.xticks(fontsize=14) plt.yticks([1,2,3,4,5,6,7,8,9,10,11], fontsize=14) plt.ylabel(&#39;Number of Splits&#39;, fontsize=14) plt.grid() plt.legend(loc=2, prop={&#39;size&#39;: 14}) plt.show() . Conclusion . As we saw above in all cases even random rainbow wins against one-hot. . numerate figures? | . Discuss assumptions . Randomness | . D: randomness might have different effects . D: in actual one-hot - a loooot of features, so randomness assumption is not far from truth. . We base our analysis on the assumption that model features are selected somewhat randomly by the model and added to the tree nodes. In other words, the model does not compare all splits across all variables before chosing the split on the current node, rather it picks a feature randomly and then picks a split value inside that single scale. We understand that this is a strong assumption and in reality the One-hot splits are not fully random. However they are also not fully deterministic. First, most of the algorithms don&#39;t run all possible comparisons within all features to make a choice about the node, rather some quick set of rough comparisons, which is done to avoid overfitting. Second, various hyperparameters have a substantial effect on which split will be chosen. Subsample of columns . randomness of One-hot they should be random to avoid overfit . hyperparameters, simple model | . Single tree? Single trees - easy assumption - equivalent to forest of depth 1 trees. | . Our method works on simpler models . Saves significant dimensionality . Not losing even a bit of information . For One-hot, assume random selection of the next choice. . subsample col | subsample rows | not all splits considered, rather some approximation even on a single scale | counts of samples in each value can be highly uneven - this worse for one-hot . | assume inside one var all splits are considered, but not across all vars . | that one-hot and rainbow might both win from minority thing . | . But what about best one-hot vs random rainbow??? . Recommendations: argument for rainbow . pick a few, choose the best | compare | compare average rainbow with best One-hot | . Bibliography and References and Technical Details . References . While the topic of One-hot and binary-One-hot encoding has been touched by many authors: . https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159 | https://towardsdatascience.com/stop-One-hot-encoding-your-categorical-variables-bbb0fba89809 | . The approach presented here is rarely used in industry or academia, while has shown superior results and by definition creates less features. .",
            "url": "https://yourdatablog.com/rainbow/",
            "relUrl": "/rainbow/",
            "date": " • Mar 15, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Detecting sounds with Deep Learning",
            "content": "Introduction . Have you ever woken up without understanding what it was, but knowing for sure that some sound isn’t right? . Sound identification is one of our instincts that kept human beings safe. Sounds play a significant role in our life, Starting from recognizing a predator nearby to being inspired by music, to lots of human voices, to the cry of a bird. Therefore, developing audio classifiers is a crucial task in our lives. . In many cases, it is crucial to classify the source of the sounds and is already widely used for various purposes. In music, there&#39;s a classifier for the genre of music. Recently similar systems started to be used to classify bird calls, something that historically was done by a profession called Ornithologists. Their goal is to categorize which sounds of birds because it is difficult to detect the birdcalls from the fields or noisy environments. . Recently, deep learning (DL) has become one of the popular technologies to solve lots of tasks in our lives due to its accuracy, along with the improvement of computational devices like CPU (Central Processing Unit), GPU (Graphics Processing Unit). The below chart shows how big the deep learning market is and the expected size of its future from the aspects of the software, hardware, and services. . . Fig 2. Deep Learning market of U.S. from 2014 to 2025 . In this post, We will take the task of reading an audio file with zero to few bird calls and use deep learning to identify which bird it is, based on the Cornell Birdcall Identification Kaggle Challenge where we’ve got a silver medal. . How to deal with the data? . In the previous post our team wrote, we explained how to load sound data and get it to a spectrogram format and why it is crucial. Here’s an example of a spectrogram of birdcalls of Alder Flycatcher and a photo of such a bird, just in case you are curious. . . Fig 3. log mel spectrogram of birdcall, Alder Flycatcher The speed of data processing is one of the keys to utilizing a deep learning model. Although the increment of computation power, the computation cost of audio processing is still expensive on CPUs. However, if we choose a better computation resource to process the data like GPUs, it can boost the speed of about ten to one hundred times faster! In this post, we will show how to process Spectrogram fast by utilizing a library called torchlibrosa that enables us to process Spectrogram on a GPU. . Build Spectrogram processor . torchlibrosa is a Python library that has some audio processing functions implemented in PyTorch that can utilize GPU resources. PyTorch enables running this Spectrogram algorithm on a GPU. Here&#39;s an example of extracting Spectrogram features using torchlibrosa. . from torchlibrosa.stft import Spectrogram spectrogram_extractor = Spectrogram( win_length=1024, hop_length=320 ).cuda() . Load audio data . We can load audio data via librosa library, which is one of the popular Python audio processing libraries. . import librosa # get raw audio data example, _ = librosa.load(&#39;img-kim/example.wav&#39;, sr=32000, mono=True) . Process Spectrogram . import torch raw_audio = torch.Tensor(example).unsqueeze(0).cuda() spectrogram = spectrogram_extractor(raw_audio) . Benchmark processing speed . We can process audio data on the GPU by using torchlibrosa library. You may wonder how much faster on the GPU than the CPU. Here&#39;s the speed of processing the benchmark between the devices. We just selected audio from the dataset obtained from the Cornell Birdcall Identification Kaggle Challenge, which is publicly available, and compared how long it takes on CPU and GPU. We tested on the Colab environment to reproduce the performance, and it is about x15 faster on GPU than CPU to process log-mel spectrogram from about 5 minutes audio. . . Fig 4. Processing time between CPU (Intel Xeon 2.20 GHz) and GPU (Nvidia T4). librosa is used for CPU benchmark, torchlibrosa is used for GPU benchmark How to classify a sound? . As mentioned above, deep learning also shows a brilliant performance in the audio domain. It can catch various patterns of target classes nicely in the time-series data. The more important thing is the environment and data matter in bird calls. The environments like fields or the middle of the mountains, there are lots of noises interfering with the birdcalls. There are lots of birds that can exist in long recorded audio. So considering these cases, we need to build a noise-robust, multi-label audio classifier. . We are going to introduce a deep learning architecture used by our team (Dragonsong) in Cornell Bird Call Identification Kaggle Challenge. . Architecture . We built a novel audio classifier architecture that catches time-series features effectively by utilizing CNN, RNN and attention modules. Here is our brief plot of architecture used at the Cornell Birdcall Identification Challenge. . . Fig 5. Our architecture of birdcall classifier We process a raw audio with a log-mel spectrogram as an input of our architecture, and it passes through the ResNeSt50 backbone, which is one of the image classification architectures. Then, we take the features, which contain both spatial and temporal information, to the RoI (Region of Interest) pooling and bi-GRU layers. In the layers, it catches the time-wise information again while reducing the feature dimension because we thought of extracting temporal features are crucial to classify lots of bird calls in long audio. Lastly, we pass the information into the attention module to score by each time step to find out which time step the birds exist. . Training the model . Not only building deep learning architecture to represent the data but also how to train the model is crucial (a.k.a training recipe). To classify audios that contain multiple bird calls in a noisy environment, we mix multiple bird calls into audio and noises like white noise. Also, regarding lots of variation of bird calls, we augment time and pitch and mask some audio frames by using SpecAugment. Here is a short example of what we applied augmentations. . import IPython.display as ipd ipd.Audio(&#39;img-kim/mixed.wav&#39;) . Your browser does not support the audio element. Fig 6. Augmented sample. The mixed version of Alder Flycatcher and American Avocet. As a result, we can achieve an outperform score on the Kaggle challenge. . Summary . Have you ever woken up without understanding what it was, but knowing for sure that some sound isn&#39;t right? With good algorithms, machines will be able to identify what it was and help you sleep better. Stay tuned! .",
            "url": "https://yourdatablog.com/audio/",
            "relUrl": "/audio/",
            "date": " • Nov 28, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Seeing is Believing",
            "content": "Introduction . Close your eyes and listen to the sound around you. No matter whether you are in a crowded office, cozy home, or open space of nature, you can distinguish the environment with sound around you. One of the five major senses of humans is hearing, so audio plays a significant role in our life. Therefore, being able to organize and exploit values in audio data with deep learning is a crucial process for AI to understand our world. An important task in sound processing is enabling computers to distinguish one sound from another. This capability enables computers to do things ranging from detecting metal wearing in power plants to monitoring and optimizing fuel efficiency of cars. In this post, we will use bird sound identification as an example. We will detect locations of bird calls in recordings produced in natural settings and classify species. By converting audio data to image data and applying computer vision models, we acquired a silver medal (top 2%) in Kaggle Cornell Birdcall Identification challenge. . Treating Audios as Images . When a doctor diagnoses heart problems, he can either directly listen to the patient’s heartbeat or look at the ECG, a diagram that describes the heartbeat, of the patient. The former usually takes longer - it takes time for the doctor to listen - and harder - memorizing what you heard can be hard. In contrast, visual perceptions of ECG allows a doctor to absorb spatial information instantly and accelerates the tasks. . The same rationales apply to our sound detection tasks. Here are four audio clips and corresponding spectrograms of four bird species. Even human eyes can see the differences between species instantly based on color and shapes. . . import IPython.display as ipd ipd.Audio(&#39;img-tony/amered.wav&#39;) . Your browser does not support the audio element. ipd.Audio(&#39;img-tony/cangoo.wav&#39;) . Your browser does not support the audio element. ipd.Audio(&#39;img-tony/haiwoo.wav&#39;) . Your browser does not support the audio element. ipd.Audio(&#39;img-tony/pingro.wav&#39;) . Your browser does not support the audio element. Going over the audio waves through time takes more computational resources, and we can acquire more information from the 2-dimensional data of images than 1-dimensional waves. In addition, the recent rapid development of computer visions, especially with the help of Convolutional Neural Network (CNNs), can significantly benefit our approach if treating audios as images as we (along with pretty much everyone) did in the competition. . Understanding Spectrogram . The specific image representation that we use is called a spectrogram: a visual representation of the spectrum of frequencies of a signal as it varies with time. . Sounds can be represented in the forms of waves, and waves have two important properties: frequency and amplitude as illustrated in the picture below. The frequency determines how the audio sounds like, and amplitude determines how loud the sound is. . . In a spectrogram of an audio clip, the horizontal direction represents time, and the vertical direction represents different frequencies. Finally, the amplitude of sounds of a particular frequency exists at a particular point of time is represented by the color of the point, resulting from the corresponding x-y coordinates. . . To more intuitively see how frequencies are embodied in spectrograms, here’s a 3D visualization, which demonstrates the amplitude with an extra dimension. Again, the x-axis is time, and y-axis is the value of frequencies. The z-axis is the amplitude of sounds of the frequency of y-coordinate at the moment of the x-coordinate. As the z-value increases, the color changes from blue to red, which results in the color we saw in the previous example of a 2D spectrogram. . . Spectrograms are helpful because they extract exactly the information we need: frequencies, the features that shape the form of sound we hear. Different bird species, or actually all objects that produce sound, have their own unique frequency range so that their sounds appear to be different for our ears. Our model will simply need to master distinguishing between frequencies to achieve ideal classification results. . Mel Scale and Mel Scale Spectrogram . However, human ears do not perceive differences in all frequency ranges equally. As frequencies increase, it is more difficult for us to distinguish between different frequencies. In order to better emulate human ear behaviors with deep learning models, we measure frequencies in mel scale. In the mel scale, any equal distance between frequencies sound equally different for human ears. mel scale converts frequency from in Hertz (f) to in mel (m) with the following equation: . . A mel scale spectrogram is simply a spectrogram with frequencies measured in mel. . How do We Use Spectrogram? . To create a mel spectrogram from audio waves, we will employ librosa library. . import librosa import numpy as np y, sr = librosa.load(&#39;img-tony/amered.wav&#39;, sr=32000, mono=True) melspec = librosa.feature.melspectrogram(y, sr=sr, n_mels = 128) melspec = librosa.power_to_db(melspec).astype(np.float32) . Where y denotes the raw wave data, sr denotes sample rate of the audio sample, and n_mels decides the number of mel bands in the generated spectrogram. When using melspectrogram method, you can also set f_min and f_max method You can also set Then, we can convert mel spectrogram that express amplitude in amplitude squared scale to decibel scale with the power_to_db method. . To visualize the generated spectrogram, run . import librosa.display librosa.display.specshow(melspec, x_axis=&#39;time&#39;, y_axis=&#39;mel&#39;, sr=sr, fmax=16000) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f49d0022b90&gt; . Alternatively, if you are using GPU, you can accelerate the mel spectrogram generation process with torchlibrosa library. . is_gpu = False #For demonstration purpose, is_gpu is set to False. Change is_gpu to True when using torchlibrosa. if is_gpu: from torchlibrosa.stft import Spectrogram, LogmelFilterBank spectrogram_extractor = Spectrogram() logmel_extractor = LogmelFilterBank() y = spectrogram_extractor(y) y = self.logmel_extractor(y) . Summary . In conclusion,we can take advantages from recent developments in computer vision in audio-related tasks by converting audio clips into image data. We achieve so with spectrograms that exhibit frequency, amplitude, and time information of audio data in an image. Using mel scale and mel scale spectrogram helps computers to emulate human hearing behaviors of distinguishing sounds of different frequencies. To generate spectrograms, we could employ librosa library, or torchlibrosa for GPU acceleration, in Python. By treating audio-related tasks in such a way, we are able to establish efficient deep learning models to identify and classify sounds, like how doctors diagnose heart-related diseases with ECG. .",
            "url": "https://yourdatablog.com/audio2images/",
            "relUrl": "/audio2images/",
            "date": " • Nov 21, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "How to parse mp3 as an array of wave data",
            "content": "In this short post we will share our experiences on working with sound data from python - applying Fourier Transfort to wave data from mp3 and use of some predefined filter. We will use FFMPEG (https://ffmpeg.zeranoe.com/builds/) which you need on your PC and in Jupyter, as well as PDUD and noise reduction technologies from (https://pypi.org/project/noisereduce/) . As usual, if you want to play with the notebook - do that on colab or github. . So, say you have a mp3 which contains data you need to process - in can be sound of a machine working, say you want to see if you can detect malfunctions from its sound - or a bird song. If you get a massive ammount of sounds - it is probably in mp3. We will load it via pydub. . #works with local data filename = &quot;../data/datasets_847828_1448297_birdvox_wav_00162d26-57e6-4924-a247-5f58963f1403.wav&quot; disk_file = open(filename, &#39;rb&#39;) sound = pydub.AudioSegment.from_file(disk_file, format=&#39;wav&#39;) samples = sound.get_array_of_samples() # get the frame rate sample_rate = sound.frame_rate # get amount of bytes contained in one sample sample_size = sound.sample_width # get channels channels = sound.channels disk_file.close() print(&quot;Channels: &quot;, channels, &quot;Width: &quot;, sample_size, &quot;Rate: &quot;, sample_rate) . Channels: 1 Width: 2 Rate: 44100 . You can see visualize it on a plot - your samples are in an array. . #create a time variable in seconds time = np.arange(0, float(len(samples)), 1) / sample_rate #plot amplitude (or loudness) over time plt.plot(time, samples, linewidth=0.01, alpha=0.7, color=&#39;#ff7f00&#39;) plt.xlabel(&#39;Time (s)&#39;) plt.ylabel(&#39;Amplitude&#39;) plt.show() . And now we can do apply fourier transform . from numpy import fft as fft fourier=fft.fft(samples) n = len(samples) fourier = fourier[0:(int(n/2))] fourier = fourier / float(n) freqArray = np.arange(0, (int(n/2)), 1.0) * (sample_rate*1.0/n); plt.plot(freqArray/1000, 10*np.log10(fourier), color=&#39;#ff7f00&#39;, linewidth=0.02) plt.xlabel(&#39;Frequency (kHz)&#39;) plt.ylabel(&#39;Power (dB)&#39;) plt.show() . C: ProgramData Anaconda3 lib site-packages numpy core _asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part return array(a, dtype, copy=False, order=order) . Pxx, freqs, bins, im = plt.specgram(samples, Fs=sample_rate, NFFT=1024, cmap=plt.get_cmap(&#39;autumn_r&#39;)) cbar=plt.colorbar(im) plt.xlabel(&#39;Time (s)&#39;) plt.ylabel(&#39;Frequency (Hz)&#39;) cbar.set_label(&#39;Intensity dB&#39;) plt.show() . Say, you want to pick a frequency and visualize its amplitude . #pick a frequency i = 250 singlefqcy=Pxx[i,:] plt.xlabel(&#39;Time (s)&#39;) plt.ylabel(&#39;Amplitude&#39;) plt.title(&#39;Amplitude for freq {:.2f}kHZ&#39;.format(freqs[i]/1000)) plt.plot(bins, singlefqcy, color=&#39;#ff7f00&#39;) . [&lt;matplotlib.lines.Line2D at 0x1f93a1d7f48&gt;] . Now, let us reduce noise using noisereduce library . import noisereduce as nr fconverter = lambda a : a / 32767.0 converted = fconverter(np.asarray(samples, dtype = np.float64)) reduced_noise = nr.reduce_noise(audio_clip=converted, noise_clip=converted, verbose=False) . C: ProgramData Anaconda3 lib site-packages noisereduce noisereduce.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console) from tqdm.autonotebook import tqdm . And check that noise is reduced on Fourier Transform results . fourier=fft.fft(reduced_noise) n = len(reduced_noise) fourier = fourier[0:(int(n/2))] fourier = fourier / float(n) freqArray = np.arange(0, (int(n/2)), 1.0) * (sample_rate*1.0/n); plt.plot(freqArray/1000, 10*np.log10(fourier), color=&#39;#ff7f00&#39;, linewidth=0.02) plt.xlabel(&#39;Frequency (kHz)&#39;) plt.ylabel(&#39;Power (dB)&#39;) plt.show() . C: ProgramData Anaconda3 lib site-packages numpy core _asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part return array(a, dtype, copy=False, order=order) . And now you can write the noise reduced file to output. . import scipy.io.wavfile scipy.io.wavfile.write(&#39;../output/test-out2.wav&#39;,sample_rate,reduced_noise) . Copyright &copy; Dmytro Karabash, Maxim Korotkov; 2020. This notebook is licensed under a Creative Commons Attribution 4.0 International License. .",
            "url": "https://yourdatablog.com/audio2/",
            "relUrl": "/audio2/",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Organizational performance explained with Game of Thrones",
            "content": "KPI, OKR, MBO, etc. Managers seem to like 3-letter abbreviations. Let&#39;s make them easy to remember. So, what is a Key Performance Indicator (KPI)? KPI&#39;s have been here for ages, and they are just about setting up quantifiable parameters and measuring those. It can be used as objectives for organization units or team members. What parameter can be measured when we look at the Battle of the Blackwater (from the side of King&#39;s Landing - Cersei seems profound in management techniques)? If you don&#39;t know what I&#39;m talking about, I suggest you close the article right here - otherwise, read on. So let&#39;s look first at the parameter she is interested in - it is the percentage of Stannis Baratheon ships destroyed by the wildfire. It is an essential parameter for the protecting army in King&#39;s Landing. And it is nicely cascaded downwards. The group target is set (while the outcome is slightly lower). . . Can this one be a Key Performance Indicator for your organization? Unless you run such battles on a recurrent basic (and they didn&#39;t do that even in Seven Kingdoms) - I would not refer to it as a KPI. You would like to use KPIs to monitor organization performance and improve it over periods, and this one is a unique project. You can have Project KPIs, but I suggest sticking with the name Project Metrics. Advanced reader note - terms you use do not matter. You can have project KPIs in your organization. What matters is that you don&#39;t confuse different management tools and use them only for what they are. Project Metrics are parameters set on a project level, measured throughout its execution, and reviewed at project retrospective or lessons learned. . Let&#39;s move to the North and set some KPIs for Night Watch. Quickly for those who&#39;ve forgotten: there are Seven Kingdoms, and the Wall built 8000 years ago. North or the Wall, there are bad things - wildling, giants, White Walkers maybe. The Night Watch aims to guard the Wall to keep the wildlings and White Walkers from crossing into the Seven Kingdoms. So, what can we measure: . . Is this a set of KPIs for the Night Watch organization? It can be. . Do organizations need KPIs? Yes, especially the bigger ones. The larger organization is, the more it will get focused on KPIs. How do you get your KPIs? You know your department goal: Consultancy unit aims to generate Profit by delivering Professional Services, Night Watch aims to kill wildling and White Walkers. Then you set up a measurement system to capture this performance and cascade it—pure, straightforward, top-down. . Now, you are a Lord Commander sitting in Castle Black, and you need to cascade the KPIs we&#39;ve mentioned above. Are they good ones? Think of them for a second. How will you measure it and make it transparent? You are a new generation Lord Commander and set up bonuses for your Leadership Team and the Watch&#39;s members. The Oath is &quot;I shall take no wife, hold no lands, father no children. I shall wear no crowns and win no glory. I shall live and die at my post...&quot; but nothing about &quot;I shall take no performance-based yearly bonus.&quot; Can you set up a yearly bonus based on the parameters above and reflect your leaders&#39; performance? Let&#39;s focus on Rangers first (they fight, defend the Wall, and patrol the Haunted Forest). Well, if you just take numbers above, every leader gets the same KPI (because those are measured globally). Moreover, how do you know how many White Walkers are out there? Adjustment required. . You get some ale, think, and develop a KPI cascade. You end up with individual KPIs like: . . The second KPI is loosely aligned with the organizational objective, and that&#39;s what typically happens in organizations. Leadership KPIs are based on their team averages (e.g., 60% of bonus) and organization KPIs (40% of bonus then). So, years pass, and your team knows that their yearly bonus depends on those KPIs. You get different types of performers. Some truly follow your Vision Statement (&quot;I am the sword in the darkness. I am the watcher on the walls. I am the shield that guards the realms of men.&quot;), but most do not. For those, you observe two behaviors (or a combination of both). Some rangers will go to patrol, take some ale (should we prohibit it during working hours?), and camp a few miles outside. They get patrol KPIs high, and Leadership closes its eyes on that - because their bonus goes up as well. Some will want to be heroes and go kill wildlings in crazy numbers, doing a lot of overtime (that&#39;s not what you&#39;ve wanted, but it helps to promote the company image and hire recruits). Some will raid into the North, kill villagers and present them as wildlings. If you find out, you classify it as not-compliance and apply sanctions (up to execution). . And such an organization works well as long as times are not too turbulent. How? Your individual KPIs are loosely aligned to the organization&#39;s objective, but they are somewhat aligned. There are a lot of camps around (you cap performance component at 200% percent, so it makes sense to patrol a lot), so you are likely to spot a White Walker. Wildlings go deeper into the woods and thus are not trying to cross the Wall either. In a more volatile environment, when you need to innovate because of changing business settings, your organization appears less efficient. . Peter Drucker, in his 1954 book The Practice of Management, proposed an approach called Management by Objectives (MBO). You&#39;re a Lord Commander, and you have not read that, but Maester referred you to something similar from a thousand years back. You don&#39;t want to rely solely on KPIs in appraisals (as too many members seem to do nearly nothing valuable and still get to 100%). You want to reward performance based on objectives that every sworn brother agrees to individually - so make this process more tailor-made. . You split your brotherhood into the Rangers, the Builders, and the Stewards with 3 Leaders (First Ranger, First Builder, and First Steward). You&#39;ve seen giants yourself, and they are enormous. So you talk to the First Builder and agree on specific objectives: . . Such objectives allow the Leadership team to cascade it further in a way that is tailored to their groups or team members, each having their strengths and weaknesses. You no longer set up the same KPIs within a unit, but you align on every individual&#39;s objective on a yearly meeting. Now you are even more competent Lord Commander - you Manage by Objectives. Brothers define objectives together with their leaders aligned with their skills and interests, and your organization grows. First Ranger also takes a more tailor-made set of objectives: . . What&#39;s the issue now? Well, typically, your Objectives are defined between a team member and his boss and are not shared around (exceptions possible), and you set them in a way that reaching 100% is possible with reasonable effort. But the White Walkers have been spotted, and you have no clue how to kill those guys. . You are in an even more volatile environment - you probably have to innovate now, and what you need is Objectives and Key Results (OKR). The concept was introduced by Andrew Grove, former CEO of Intel, in the early 1980s. OKRs were popularized by Google using them from the beginning (you have no clue who Andrew Grove is, probably a wise Maester from the past). OKRs are similar to Objectives you used to set up before, with few key differences: . OKRs are transparent (everyone can see all OKRs) | OKRs are set to be challenging (so reaching 100% or all of them is hard) - 70% is already good | OKRs are reviewed more often (quarterly) | OKRs specify not only &quot;what&quot; (Objective) but also &quot;how&quot; (Key Result) | OKRs are not linked to compensation (Bullshit statement - indeed your boss knows how you are doing, so they may not be linked directly, but your package depends on how well you perform on OKRs) You could do everything from above within the MBO framework and never mention an OKR. OKR is a specific way to set up objectives and some adjustments to a technique, not something to replace Management by Objectives entirely. | . Now we are talking. You Maester gets an OKR for this quarter, and your First Ranger also gets one. . . Those are not that easily reachable and will put quite some challenges. In most organizations, you get OKRs where a percentage of completion can be measured, but you can also get a list of more qualitative ones. So now, you can handle innovation better, and your organization strives to grow from inside. Just don&#39;t forget your Vision Statement &quot;I am the fire that burns against the cold, the light that brings the dawn, the horn that wakes the sleepers, the shield that guards the realms of men.&quot; . Copyright &copy; Maxim Korotkov; 2020. This article is published under a Creative Commons Attribution 4.0 International License. .",
            "url": "https://yourdatablog.com/kpisandokrs/",
            "relUrl": "/kpisandokrs/",
            "date": " • Jun 6, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Use Data Science to Handle Enterprise Resource Planning Data with Ease",
            "content": "Introduction . You’re a boss now. You have a team or a business unit doing consultancy. Probably a bunch of Consultants helping clients, some Project Managers leading your projects in a nice Agile or whatever way. Everyone fills their timesheets (and you cut their bonuses if they don’t) – the time is billed to clients, and you have Fixed Fee projects as well. Maybe smaller teams being parts of your organization also – you can lead ten, a hundred or a thousand people (hey, dude – you run a unit of 1000 employees? Don’t read this article – hire someone who did). You probably even have an ERP (Enterprise Resource Planning) or something and a CFO in a lovely corner office. Do you have an excellent way to estimate how much profit every team member and project brings and ways to predict it with reasonable precision? If you have a decent ERP dashboard which gives you all that – then you’re lucky, and you don’t need this article either. The chances are that you get a simple calculation end of the year – like “let’s take all revenue finance recognized on your projects in a year and man day cost at USD 800 to get your unit profitability”. USD 800 can seem crazily high or unacceptably low, depending on the business you’re in. So that’s the precision you have to count the money your team brings to the firm? Do we need to remind you that this is one of the reasons why your job exists? . What else can you get? All the timesheets projects with their budgets, and you can even get to approximate costs (let’s discuss it later) — a shitload of data for a year, even for a team of 10. What we show is that you don’t need an ERP to do the rest – the notebook in Jupyter will do. Keep this in mind - management starts with measurement. How can you supervise your team and projects if you don’t have their data at your fingers? . Why do you need Data Science here? Well, you can have all data at your fingers. Eventually, you would want to calculate your profit like . profit = revenue - cost . Not only as subtracting two numbers, but also on the level of tables - so the output of the above statement can be a table with profit per consultant per month, like this: . profit . 2020-02 2020-03 . CATHY THE NEW MANAGER 6187.50 | 8006.25 | . FRANK THE CONSTULANT 8050.00 | 3762.50 | . Or get a list of 3 people with most billable hours in March via the following statement . t.where(t[&#39;Billable&#39;] &amp; (t[&#39;month&#39;] == &#39;2020-03&#39;) ).groupby([&#39;User&#39;])[&#39;effort&#39;].sum().nlargest(3) . The code above is indeed not executable, but we promise to get you there in a few paragraphs. And yes, there is some learning of python required. The huge difference from anything else you’ve been doing yourself (ERP reports, Excel, other tools) is that any analysis stays in the notebook and can be re-applied after your data changes. . Data Science . So, let’s get this done. First – yes, you need to know a bit of python to get the files in. The basic level will do. If you are a manager in 2020 and can’t write a simple script – hmm, you probably missed something. The objective is not to build an ERP and not even to have an easy-to-use toolkit - we want to show you how to make a toolkit for yourself (but feel free to reuse our code). What you will see is a straightforward set of examples - spend an hour or two to load your files into the notebook and then start playing with it - just make the analysis you want. You can end up checking a few numbers or building your charts and dashboards. It is ok, even if you work for a corporation (that’s where you need it most) - just install Anaconda and download the notebook. So, we first get and transform our input files (if you want to follow text AND all the code - read it on colab). . We are loading projects, timesheets, and roles below - more details will follow on what we presumed is inside. Note - files are loaded from the GitHub repository so that they will open in colab or your local machine just the same. . data = {} data_path = &#39;https://raw.githubusercontent.com/h17/fastreport/master/data/&#39; url = data_path + &#39;roles.csv&#39; data[&#39;roles&#39;] = pd.read_csv(url, index_col=0, sep=&quot;;&quot;) url = data_path + &#39;project_data.xlsm&#39; project_data = pd.ExcelFile(url) project_data.sheet_names . [&#39;TS Feb&#39;, &#39;TS Mar&#39;, &#39;TS Apr&#39;, &#39;Projects&#39;, &#39;Employees&#39;] . Don’t focus too much on the next code block - we have monthly timesheets in separate tabs and need to stack them one on top of another. Plus - we have errors in excel (Sat.1 column) - data cleanup is also required, quite usual for those who touched data science. Collapsed blocks in our notebook contain the code, which is not critical for understanding. If you’d rather read it all, we suggest you switch to either GitHub or colab, so you can also play with it. . # collapse timesheets = [] for sheet in project_data.sheet_names: if &#39;TS&#39; in sheet: timesheets += [pd.read_excel(project_data, sheet, header=2)] else: tmp = pd.read_excel(project_data, sheet) data[sheet] = tmp if &#39;Sat.1&#39; in timesheets[0]: # cleaning from Sat.1 timesheets[0] = timesheets[0].rename( columns={&#39;Sat&#39;: &#39;Sun&#39;, &#39;Sat.1&#39;: &#39;Sat&#39;}) data[&#39;timesheet&#39;] = pd.concat(timesheets, sort=False) d = {&#39;Billable&#39;: True, &#39;Non-Billable&#39;: False} data[&#39;timesheet&#39;][&#39;Billable&#39;] = data[&#39;timesheet&#39;][&#39;Billing&#39;].replace(d) data[&#39;timesheet&#39;].drop([&#39;Billing&#39;], axis=1, inplace=True) data[&#39;timesheet&#39;] = data[&#39;timesheet&#39;][~data[&#39;timesheet&#39;].User.isnull()] . . So, let&#39;s see what we&#39;ve got here: . data.keys() for key in data.keys(): string_format = &quot;{:20} shape: {:&gt;10} rows t and {:&gt;10} columns&quot; print(string_format.format(key, *data[key].shape)) . dict_keys([&#39;roles&#39;, &#39;Projects&#39;, &#39;Employees&#39;, &#39;timesheet&#39;]) . roles shape: 8 rows and 4 columns Projects shape: 48 rows and 10 columns Employees shape: 35 rows and 5 columns timesheet shape: 1792 rows and 14 columns . Data we loaded . Let us summarize it here – most probably if you are a team leader or business unit manager – you can get . Dictionaries Cost per region or default cost for “outside” contributors | Average revenue per hour for projects not “owned” by you | . | . # show roles roles = data[&quot;roles&quot;] roles . Seniority EU US UK . Position . Unit Head SENIOR | 1100 | 1300 | 1200 | . Program Manager EXPERIENCED | 900 | 1000 | 900 | . Project Manager SENIOR | 800 | 800 | 800 | . ... ... | ... | ... | ... | . Consultant SENIOR | 800 | 800 | 900 | . Consultant EXPERIENCED | 550 | 550 | 550 | . Consultant JUNIOR | 500 | 500 | 500 | . 8 rows × 4 columns . We need to set default values and bring it to format easy to use, which is very easy in python: . default_revenue = 1200 default_cost = 850 # wide to long format roles_long = pd.melt(roles.reset_index(), id_vars=[&#39;Position&#39;, &#39;Seniority&#39;], var_name=&#39;region&#39;, value_name=&#39;cost&#39;) roles_long . Position Seniority region cost . 0 Unit Head | SENIOR | EU | 1100 | . 1 Program Manager | EXPERIENCED | EU | 900 | . 2 Project Manager | SENIOR | EU | 800 | . ... ... | ... | ... | ... | . 21 Consultant | SENIOR | UK | 900 | . 22 Consultant | EXPERIENCED | UK | 550 | . 23 Consultant | JUNIOR | UK | 500 | . 24 rows × 4 columns . HeadCount List of your employees, with their grades (or costs) | Engagement model (employee or contractor) | . | . # show head count headcount = data[&quot;Employees&quot;] headcount = headcount.merge( roles_long[[&#39;Position&#39;, &#39;Seniority&#39;, &#39;region&#39;, &#39;cost&#39;]], how=&#39;left&#39;, left_on=[&#39;Seniority&#39;, &#39;Position&#39;, &#39;Country&#39;], right_on=[&#39;Seniority&#39;, &#39;Position&#39;, &#39;region&#39;]) headcount[&#39;cost&#39;] = headcount[&#39;cost&#39;].fillna(default_cost) headcount . Name Engagement Country Position Seniority region cost . 0 JANE DOE | STAFF | US | Consultant | EXPERIENCED | US | 550.00 | . 1 JOHN DOE | STAFF | FR | Consultant | JUNIOR | NaN | 850.00 | . 2 JOHN SMITH | STAFF | US | Consultant | EXPERIENCED | US | 550.00 | . ... ... | ... | ... | ... | ... | ... | ... | . 32 JOHN THE CONSULTANT | STAFF | FR | Consultant | EXPERIENCED | NaN | 850.00 | . 33 JANE THE MANAGER | CONTRACTOR | BE | Project Manager | SENIOR | NaN | 850.00 | . 34 JACK THE EXPERT | STAFF | BE | Consultant | SENIOR | NaN | 850.00 | . 35 rows × 7 columns . Projects with budgets, effort estimates, dates, types of revenue recognition (Time and Material, Fixed Fee or something else), et cetera | . # show projects projects = data[&quot;Projects&quot;] projects . Client ID Project ID Project PM Region Start End Funding Daily Rate TCV . 0 13154 | 68454 | Medium consutancy 1 | SUSAN THE MANAGER | Europe | 2019-01-16 08:00:00 | 2020-07-22 14:00:00 | Time and Materials | 1200 | 39,000 USD | . 1 2764 | 70285 | Our biggest project | EXT 6a8e0ca747 | Europe | 2019-10-01 08:00:00 | 2021-04-30 11:00:00 | Time and Materials | 1200 | 1,450,000 USD | . 2 12916 | 68093 | Upgrade to new version | TIM THE LEAVER | Europe | 2018-09-28 08:00:00 | 2020-04-27 17:00:00 | Fixed Fee | 1200 | 127,500 USD | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 45 12899 | 63852 | Huge consultancy pool | JACK THE MANAGER | Europe | 2017-08-01 08:00:00 | 2020-04-27 17:00:00 | Time and Materials | 1200 | 198,000 USD | . 46 12901 | 71248 | Small upgrade | JACK THE MANAGER | Europe | 2020-01-29 08:00:00 | 2020-12-31 17:00:00 | Time and Materials | 1200 | 108,000 USD | . 47 17719 | 70618 | Small consultancy pool | SUSAN THE MANAGER | Europe | 2019-11-06 08:00:00 | 2020-06-30 17:00:00 | Time and Materials | 1200 | 27,200 USD | . 48 rows × 10 columns . Timesheets for your team / your projects. It is possible that other teams can contribute to your projects and your team might work in outside projects Typically arrive monthly or weekly | (optional) Planned effort There might be none, in one of the next posts we&#39;ll talk about autofill | . | . | . # show timesheets timesheet = data[&quot;timesheet&quot;] timesheet . Project Activity User Period Starting Effort Type Total (Days) Sun Mon Tue Wed Thu Fri Sat Billable . 0 PRJ f74fe3647d | Consultancy | EXT 800982ab08 | 2020-02-02 | Actual | 2.50 | 0.00 | 0.50 | 0.50 | 0.50 | 0.50 | 0.50 | 0.00 | True | . 1 PRJ f74fe3647d | Consultancy | EXT c8c92ca432 | 2020-02-02 | Actual | 2.50 | 0.00 | 0.50 | 0.50 | 0.50 | 0.50 | 0.50 | 0.00 | True | . 2 PRJ f74fe3647d | Consultancy | EXT c8c92ca432 | 2020-02-09 | Actual | 2.50 | 0.00 | 0.50 | 0.50 | 0.50 | 0.50 | 0.50 | 0.00 | True | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 620 PRJ 67df00a7e9 | Consultancy | EXT ca02603616 | 2020-04-12 | Actual | 1.00 | 0.00 | 0.50 | 0.00 | 0.50 | 0.00 | 0.00 | 0.00 | True | . 621 PRJ 67df00a7e9 | Consultancy | EXT ca02603616 | 2020-04-19 | Actual | 1.00 | 0.00 | 0.50 | 0.00 | 0.00 | 0.50 | 0.00 | 0.00 | True | . 622 PRJ 67df00a7e9 | Consultancy | EXT ca02603616 | 2020-04-26 | Reported | 1.50 | 0.00 | 0.50 | 0.50 | 0.00 | 0.50 | nan | nan | True | . 1792 rows × 14 columns . EXT users here are outside of our headcount (let’s say we are not getting the real names, but rather some ids from ERP in such case) . Why not just have things done by ERP? Your ERP probably does not present you anything nicely in terms of parameters you’d want to control on your level (otherwise, you won’t read this anyway). There may be a project running to improve it by 2025 - maybe one of the Big4 firms is interviewing you for requirements. If you are steering this boat long enough – you might have ended up with quick and dirty calculation, SQL queries to SAP (with field names still in German), or an Excel file. Why? Your bosses don’t care – they already hired you, and a better ERP is coming in 2025 anyway. So they want to know how much money your team makes (preferably - per person, per month, per project, with charts, projections, and comparisons) and to know why it is not so profitable (because it never is). . To simplify your way forward we are going to create a timeseries out of timesheets which is a bit more involved so you can skip it for now and come back to it later, but the point is that at the end you will get a lovely monthly pandas DataFrame that looks like this . # collapse dd = [] timesheet[&#39;Period Starting&#39;] = pd.to_datetime(timesheet[&#39;Period Starting&#39;]) weekdays = [&#39;Sun&#39;, &#39;Mon&#39;, &#39;Tue&#39;, &#39;Wed&#39;, &#39;Thu&#39;, &#39;Fri&#39;, &#39;Sat&#39;] for i, weekday in enumerate(weekdays): columns = [col for col in timesheet if col not in weekdays or col == weekday] tmp = timesheet[columns].copy() tmp = tmp.rename(columns={weekday: &#39;effort&#39;}) tmp[&#39;date&#39;] = tmp[&#39;Period Starting&#39;]+pd.to_timedelta(&#39;{}D&#39;.format(i)) tmp[&#39;weekday&#39;] = weekday tmp = tmp.drop([&#39;Period Starting&#39;, &#39;Total (Days)&#39;], axis=1) dd += [tmp] timeseries = pd.concat(dd, sort=False).sort_values([&#39;User&#39;, &#39;date&#39;]) timeseries = timeseries.reset_index().drop(&#39;index&#39;, axis=1) timeseries[&#39;date&#39;] = pd.to_datetime(timeseries[&#39;date&#39;]).astype(&#39;str&#39;) timeseries[&#39;month&#39;] = timeseries[&#39;date&#39;].str[:7] def isweekend(x): return x in [&#39;Sun&#39;, &#39;Sat&#39;] timeseries[&#39;weekend&#39;] = timeseries[&#39;weekday&#39;].apply(isweekend) timeseries[&#39;workweek&#39;] = ~timeseries[&#39;weekday&#39;].apply(isweekend) timeseries . . Project Activity User Effort Type effort Billable date weekday month weekend workweek . 0 Upgrade in two contracts - 1 | Consultancy | CATHY THE NEW MANAGER | Actual | 0.00 | True | 2020-02-16 | Sun | 2020-02 | True | False | . 1 Upgrade in two contracts - 2 | Consultancy | CATHY THE NEW MANAGER | Actual | 0.00 | True | 2020-02-16 | Sun | 2020-02 | True | False | . 2 A pilot for huge opportunity | Consultancy | CATHY THE NEW MANAGER | Actual | 0.00 | True | 2020-02-16 | Sun | 2020-02 | True | False | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 12541 Strategic project in a new region | Project Management | TOM THE TEAM LEADER | Reported | nan | True | 2020-05-01 | Fri | 2020-05 | False | True | . 12542 Strategic project in a new region 2 | Consultancy | TOM THE TEAM LEADER | Actual | nan | False | 2020-05-02 | Sat | 2020-05 | True | False | . 12543 Strategic project in a new region | Project Management | TOM THE TEAM LEADER | Reported | nan | True | 2020-05-02 | Sat | 2020-05 | True | False | . 12544 rows × 11 columns . It is common in data science to have one big DataFrame (similar to excel tab or SQL table) that you can group and filter easily. It is helpful because it makes things very easy. . Profitability . Timesheets and project budgets are simplified this way, and you can manipulate them the way you want in a few simple statements. If you studied excel formulas - you could do this also. Now, let’s look at profitability in the most straightforward manner possible. As a manager, we suggest you do that to set benchmarks for your team. So, profit is $$ mbox{Revenue  –  Cost}$$ and we intend to keep it simple. You can implement the calculations you like. . Revenue: Let’s say that for a Time &amp; Material project, you generate as much revenue as you bill (on agreed rate) up to its budget. We are not doing any Fixed Fee recognition formula. Different companies do it differently, and you’ll either need historical data or actual recognized value depending on how you operate. . We also assume that we are interested only by users in our headcount (so we filter out all EXT users). . # revenue calculation for a person for a month # (SUM REV of all timesheet records * corresp project rates) revenue_df = timeseries[[&#39;User&#39;, &#39;Project&#39;, &#39;Billable&#39;, &#39;effort&#39;, &#39;date&#39;, &#39;month&#39;]] revenue_df = revenue_df.merge(projects[[&#39;Project&#39;, &#39;Daily Rate&#39;]], how=&#39;left&#39;, on=&#39;Project&#39;) revenue_df[&#39;Daily Rate&#39;].fillna(default_revenue, inplace=True) revenue_df = revenue_df[(revenue_df[&#39;User&#39;].str[:3] != &#39;EXT&#39;) &amp; (revenue_df[&#39;Billable&#39;])] revenue_df[&#39;daily_revenue&#39;] = revenue_df[&#39;Daily Rate&#39;] * revenue_df[&#39;effort&#39;] revenue = revenue_df.groupby([&#39;User&#39;, &#39;month&#39;])[ &#39;daily_revenue&#39;].sum().unstack().fillna(0) revenue = revenue[revenue.sum(1) &gt; 0] revenue.head() . month 2020-02 2020-03 2020-04 2020-05 . User . CATHY THE NEW MANAGER 11550.00 | 19350.00 | 9412.50 | 0.00 | . FRANK THE CONSTULANT 15750.00 | 12425.00 | 14875.00 | 0.00 | . IVAN THE PROGRAMMER 25062.50 | 29043.75 | 25643.75 | 0.00 | . JACK THE EXPERT 29093.75 | 31331.25 | 24843.75 | 0.00 | . JACK THE MANAGER 24090.62 | 22318.75 | 18821.88 | 0.00 | . We got revenue per month per person. Not too sophisticated, right? . Cost: Let’s start with the fact that just using the “default cost rate” is not enough - whenever margins are under pressure, you should do better. You might have people working in different countries and of totally different levels. Talk to your finance team and get some estimates from them (or make it together). We’d say they owe you that much if you were appointed to run a team or business unit. We will assume below that you managed to get a few grades per Country (see roles). Costs per individual will be nice to have as well. The beauty of this tool (compared to doing Excel by yourself) is that you can just add it - it will be a few lines of code. Let’s calculate direct cost per month: here we check the cost of effort posted on timesheets, presuming they are full - can check per number of working days in a month also and compare. We are not interested in external resources, so we’ll filter them out again. . # cost calculation for a project # (SUM COST of all timesheet records * corresp cost rates - see roles) cost_df = timeseries[[&#39;User&#39;, &#39;Project&#39;, &#39;effort&#39;, &#39;date&#39;, &#39;month&#39;]] cost_df = cost_df.merge(headcount[[&#39;Name&#39;, &#39;cost&#39;]], how=&#39;left&#39;, left_on=&#39;User&#39;, right_on=&#39;Name&#39;) cost_df = cost_df[cost_df[&#39;User&#39;].str[:3] != &#39;EXT&#39;] cost_df[&#39;daily_cost&#39;] = cost_df[&#39;cost&#39;] * cost_df[&#39;effort&#39;] cost = cost_df.groupby([&#39;User&#39;, &#39;month&#39;])[&#39;daily_cost&#39;].sum() cost = cost.unstack().fillna(0) cost = cost[cost.sum(1) &gt; 0] cost.head() . month 2020-02 2020-03 2020-04 2020-05 . User . CATHY THE NEW MANAGER 5362.50 | 11343.75 | 9332.81 | 0.00 | . FRANK THE CONSTULANT 7700.00 | 8662.50 | 7768.75 | 0.00 | . IVAN THE PROGRAMMER 11481.25 | 12925.00 | 11618.75 | 0.00 | . JACK THE EXPERT 19921.88 | 21621.88 | 18168.75 | 0.00 | . JACK THE MANAGER 11800.00 | 10900.00 | 9600.00 | 0.00 | . Now, we can get to profit per user per month using operations on dataframes. And here it bears some fruit. Profit = revenue - cost. Indeed, it required some data cleanup first - but not too much . profit = revenue - cost profit.head() . month 2020-02 2020-03 2020-04 2020-05 . User . CATHY THE NEW MANAGER 6187.50 | 8006.25 | 79.69 | 0.00 | . FRANK THE CONSTULANT 8050.00 | 3762.50 | 7106.25 | 0.00 | . IVAN THE PROGRAMMER 13581.25 | 16118.75 | 14025.00 | 0.00 | . JACK THE EXPERT 9171.88 | 9709.38 | 6675.00 | 0.00 | . JACK THE MANAGER 12290.62 | 11418.75 | 9221.88 | 0.00 | . That&#39;s what we&#39;ve promised, right? Ok, second one - people who entered most Billable hours in March . t = timeseries # concatenating just in case you are reading from phone t.where(t[&#39;Billable&#39;] &amp; (t[&#39;month&#39;] == &#39;2020-03&#39;) ).groupby([&#39;User&#39;])[&#39;effort&#39;].sum().nlargest(3) . User TOM THE EXPERT 26.75 JACK THE EXPERT 25.31 PHILIP THE EXPERT 24.88 Name: effort, dtype: float64 . What else? . Now let’s look at how to apply some python and data science techniques (we will get to more details in our next posts) to data you’ve seen above and how to visualize it nicely. . First, let’s take a PM and visualize revenue on his/her projects per month. . %matplotlib inline pm_selected = &quot;CATHY THE NEW MANAGER&quot; drawdt = revenue.loc[pm_selected].T plt.bar(range(len(drawdt.index)), drawdt.values, color=&quot;green&quot;, width=1, align=&#39;center&#39;, edgecolor=&#39;black&#39;); plt.xticks(range(len(drawdt.index)), drawdt.index); plt.ylabel(&quot;Revenue / month: &quot;+pm_selected); . That was simple. Then, some fun for those who know slightly more advanced python - you can make an interactive chart in few lines, e.g., here we want to make it visual in terms of if the value is above or below the benchmark (works if you copy notebook, is not clickable on the blog) . # collapse %matplotlib notebook # colors of bars def get_colors(v): colors = [] for i in range(len(drawdt.index)): color = &quot;yellow&quot; if (drawdt[i] &lt; v*0.9): color = &quot;red&quot; if (drawdt[i] &gt; v*1.1): color = &quot;green&quot; colors.append(color) return colors # plot drawing def plt_all(isdef, yvalue): if isdef: bcolors = &quot;white&quot; xtitle = &#39;Set value&#39; else: a = np.empty(len(drawdt.index)) a.fill(yvalue) plt.plot(range(len(drawdt.index)), a, color=&quot;black&quot;, alpha=0.5) bcolors = get_colors(yvalue) xtitle = &#39;At ${:.0f}/m is&#39;.format(yvalue) plt.bar(range(len(drawdt.index)), drawdt.values, color=bcolors, width=1, align=&#39;center&#39;, edgecolor=&#39;black&#39;) plt.xticks(range(len(drawdt.index)), drawdt.index) plt.ylabel(&quot;Revenue / month: &quot;+pm_selected) red_patch = mpatches.Patch(color=&#39;red&#39;, label=&#39;loss&#39;) yellow_patch = mpatches.Patch(color=&#39;yellow&#39;, label=&#39;in 10%&#39;) green_patch = mpatches.Patch(color=&#39;green&#39;, label=&#39;profit&#39;) plt.legend(handles=[red_patch, yellow_patch, green_patch], loc=&#39;upper left&#39;, title=xtitle, fontsize=7) plt.gca().set_title(&#39;Click on the plot to set benchmark&#39;) plt_all(True, 0.0) def onclick(event): plt.cla() plt_all(False, event.ydata) plt.gcf().canvas.mpl_connect(&#39;button_press_event&#39;, onclick); . . Ok, as a Team Leader, you might not do that. Sorry. Let’s get some calculations done. First, let’s identify “suspicious” time entered (e.g., a person who has more than three days in a row same effort on a given project and it is not 8 hours) - this is a quick check you do yourself, without asking PMO anything and making this official. I can call this suspicious because nature of our work (and yours might be different - so you look for another pattern) makes it highly unlikely that you do spend the same amount of time on one project a few days in a row (unless you are assigned full time). What you are doing is likely just splitting your working time between your projects in some manner. . # remove the weekend working = timeseries[(timeseries[&#39;workweek&#39;] == True) &amp; (timeseries.Billable)].copy() working = working.groupby([&quot;User&quot;, &quot;Project&quot;, &quot;date&quot;]).sum().sort_index() working[&#39;value_grp&#39;] = (working.effort.diff(1) == 0).astype(&#39;int&#39;) def streak(df): # function that finds streak of 1s: 0,1,1,0,1 -&gt; 0,1,2,0,1 df0 = df != 0 return df0.cumsum()-df0.cumsum().where(~df0).ffill().fillna(0).astype(int) working[&#39;streak&#39;] = streak( working[&#39;value_grp&#39;]) # streak of identical effort result = working[(0 &lt; working.effort) &amp; (working.effort &lt; 1) &amp; (working[&#39;streak&#39;] &gt; 3)].reset_index() result = result[result.User.str[:3] != &#39;EXT&#39;].groupby([&#39;User&#39;, &#39;Project&#39;]).last() result[[&quot;effort&quot;,&quot;date&quot;,&quot;streak&quot;]] . effort date streak . User Project . CATHY THE NEW MANAGER Upgrade in two contracts - 1 0.06 | 2020-03-12 | 6 | . FRANK THE CONSTULANT Upgrade in two contracts - 1 0.12 | 2020-03-05 | 4 | . Upgrade in two contracts - 2 0.12 | 2020-03-05 | 4 | . JACK THE EXPERT Some upgrade project 0.12 | 2020-03-13 | 4 | . JOHN THE CONSULTANT PRJ f8b96bd2c4 0.25 | 2020-04-28 | 6 | . TIM THE LEAVER Big upgrade and rework 0.12 | 2020-03-09 | 4 | . Upgrade to new version 0.12 | 2020-03-10 | 4 | . To be clear, we do not recommend sending emails with the subject “The list of those whose timings are suspicious” based on the above. People will likely change their behaviors, and you might not easily find the next pattern. As a manager, you dig into your data, find insights, and act on them the way you see fit. You do not just tell your teenage kid that now you know where he hides cigarettes, do you? . Use cases . Here are some cases where the above can be helpful - we will look at some of them in our next posts. . Decision-making - e.g. identify top loss-making projects | Identify projects which require management attention - also apply Machine Learning here and identify the projects you&#39;ll pick up yourself | Better analysis of non-billable hours | Identify suspicious behaviors - anomaly detections | Revenue and effort projections based on existing patterns and not highlighting when the plan deviates | Consolidated analytic on demand (e.g. profitability forecast, revenue forecast, unallocated capacity) in case your ERP will not do anything like that | . The point is that loading your data (excel, CSV, TSV - whatever) is simple, and manipulating it is simple - more straightforward than doing it in many excel files and faster than waiting for PMOs. . Stay tuned for our next post. . Copyright &copy; Dmytro Karabash, Maxim Korotkov; 2020. This notebook is licensed under a Creative Commons Attribution 4.0 International License. .",
            "url": "https://yourdatablog.com/teamdata/",
            "relUrl": "/teamdata/",
            "date": " • Jun 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Us",
          "content": "Data Science and Management by Dmytro Karabash . Management and Data Science by Maxim Korotkov . . Have something to add or a comment? Contact us via email . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://yourdatablog.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": "Welcome here. Here is our blog about applying Data Science to Management and related topics. Things you used to rely on ERP or on your PMO doing a report - how those things can be made simple with python, pandas, and jupyter. . POSTS .",
          "url": "https://yourdatablog.com/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  
  

  
      ,"page5": {
          "title": "Tags",
          "content": "{% if site.categories.size &gt; 0 %} . Contents . {% assign categories = “” | split:”” %} {% for c in site.categories %} {% assign categories = categories | push: c[0] %} {% endfor %} {% assign categories = categories | sort_natural %} . {% for category in categories %} {{ category }} | {% endfor %} . {% for category in categories %} &lt;h3 id =&quot;{{ category }}&quot;&gt;&lt;/i&gt; {{ category }}&lt;/h3&gt; {% for post in site.categories[category] %} {% if post.hide != true %} {%- assign date_format = site.minima.date_format | default: “%b %-d, %Y” -%} &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot;&gt;{{post.title}} • {{ post.date | date: date_format }}&lt;/p&gt; &lt;/article&gt; {% endif %} {% endfor %} {% endfor %} . {% endif %} .",
          "url": "https://yourdatablog.com/categories/",
          "relUrl": "/categories/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yourdatablog.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}