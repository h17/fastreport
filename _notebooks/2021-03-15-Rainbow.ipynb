{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Variables. The Rainbow Method !!!DRAFT\n",
    "> 'When the nature gives you a rainbow, take it.'\n",
    "\n",
    "- author: Anna Arakelyan, Dmytro Karabash\n",
    "- categories: [python, data science, classification, encoding]\n",
    "- image: /audio2images/\n",
    "- permalink: /rainbow/\n",
    "- hide: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"I have 2000 features and I need to get to 50 features and keep model as good or better\" was how this all started.  This is familiar to anyone in institutional data science where you need to carefully vet every model and look at each feature to make sure it is fine from the standpoint of regulation.  What is valuable about this story is that it is based on a real production model that was developed by Anna Arakelyan at Mass Mutual and a method suggested by Dmytro Karabash.  We will get to detailed results at the end but let us give us some preview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>kappa</th>\n",
       "      <th>macro-auc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num_features</th>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10</th>\n",
       "      <th>one_hot</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainbow</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">50</th>\n",
       "      <th>one_hot</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainbow</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">100</th>\n",
       "      <th>one_hot</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainbow</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      macro-f1  accuracy  kappa  macro-auc\n",
       "Num_features method                                       \n",
       "10           one_hot      0.27      0.39   0.15       0.68\n",
       "             rainbow      0.33      0.41   0.19       0.70\n",
       "50           one_hot      0.33      0.42   0.20       0.70\n",
       "             rainbow      0.34      0.42   0.21       0.71\n",
       "100          one_hot      0.33      0.42   0.20       0.71\n",
       "             rainbow      0.35      0.42   0.21       0.71"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "\n",
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Add 'rainbow_research.csv' and 'rainbow_research_feature_selection.csv' in the _notebooks folder\n",
    "# Don't push data to repo - they must be ignored in .gitignore\n",
    "results = pd.read_csv('../data/rainbow_research.csv').rename(columns={'feature_selection_parameter':'Num_features',\"transform_parameter\": \"method\"})\n",
    "\n",
    "feature_selection = pd.read_csv('../data/rainbow_research_feature_selection.csv').rename(columns={'feature_selection_parameter':'Num_features',\"transform_parameter\": \"method\"})\n",
    "metrics=['accuracy', 'kappa', 'macro avg_f1-score', 'weighted avg_f1-score', 'macro avg_roc_auc', 'weighted avg_roc_auc']\n",
    "tmp = feature_selection.groupby(['Num_features', 'method'])[metrics].mean().round(4).reset_index()\n",
    "\n",
    "tmp['Num_features']=tmp['Num_features'].str.replace('top_',\"\").astype(int)\n",
    "table1 = tmp.set_index(['Num_features', 'method']).sort_index()\n",
    "\n",
    "table1=table1.rename(columns={'macro avg_f1-score': 'macro-f1',\n",
    "                       'weighted avg_f1-score':'weighted-f1',\n",
    "                       'macro avg_roc_auc':'macro-auc',\n",
    "                       'weighted avg_roc_auc' : 'weighted-auc'\n",
    "                      })\n",
    "table1[['macro-f1','accuracy','kappa','macro-auc']].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rainbow method outperforms for any number of features as well as giving better result for 50 features than one-hot encoding for 100 . Please also note that drop decrease in kappa if you use one-hot is 5 times that of rainbow method ( 3 times for kappa and micro-auc, 2 times for accuracy).\n",
    "\n",
    "(Note*: Both models actually keep improving if you remove features smart to 100).\n",
    "!!!would be nice to add such a chart to show this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data encoding is the crucial part of any data science pipeline. No matter what is the final goal or which machine learning algorithm will be used - there is no avoiding of data massaging, cleaning, and encoding. Given that real data sets are rarely as clean and suited for analysis as toy data sets from data science classes, there is a number of decisions you have to make to encode and engineer features in the most appropriate and efficient way.\n",
    "While the encoding of quantitative and binary columns is generally straightforward, the encoding of categorical variables deserves a deeper look. \n",
    "<br><br>\n",
    "One of the most popular encoding methods for categorical variables has been the One-Hot procedure. It generates an indicator variable for every category and, thus, creates a set of K new features, where K is the number of categories.\n",
    "One significant implication of this method is the dimensionality increase. Depending on the magnitude of K, it can have various undesirable technical consequences: substantial raise of computational complexity, loss of degrees of freedom (N-K, where N is the number of samples), multicollinearity, and, ultimately, an overfitted model. Non-technical consequences include unnecessary complication of the model that should be communicated to final users and that contradict the law of parsimony, also known as Occam's razor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call it rainbow from a very simple analogy, if you have categorical feature with names \"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"indigo\" and \"violet\", in other words colors of the rainbow, instead of doing 1-hot encoding or (binary 1-hot encoding), you can simply use encoding red -> 0, orange ->1,..., violet -> 6.  This would replace seven 1-hot features with one rainbow.  The method also suggests that you can actually use two rainbows, one natural as described before and the other say by hue or warmth.  You can even use three rainbows but I wouldn't use more than $log_2(N)$ if you have N categories.\n",
    "\n",
    "Why is rainbow better than any other choice? It is first thing that comes to mind and you do not have to think about it.  In many cases it is not really that necessary which rainbow you choose (and by this we mean ordering of these colors), most of the would be better than using 1-hot, there are just some natural ones, which are more likely to be better than other (and easier to remember). \n",
    "\n",
    "\"When a nature gives you a rainbow, take it...\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!Remove:\n",
    "Instead of One-Hot, we propose an alternative way to encode categorical variables. Imagine that instead of K features you would make only one. The trick is to treat categorical variables as quantitative and find a certain way to order categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Important note here: this method is highly efficient in conjunction with the models that rely on variables as ranks rather than exact values. For example, decision trees, random forest, gradient boosting - these algorithms will output the same result if the variable *Number of Children* is coded as [1, 2, 3, 4, 5, ...] or as [-100, -85, 0, 10, 44, ...] as long as the categories have a correct order, and we use correct interpretation of non-meaningful values.\n",
    "The application of our method for other algorithms such as Linear Regression, Logistic Regression is out of scope of this article. We expect that such way of feature engineering would be still beneficial, but this is a subject of a different investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application of our method depends highly on the level of measurement. While **quantitative** variables have a *ratio* scale, i.e. they have a meaningful 0, ordered values, and equal distances between values; **categorical** variables usually have either *interval*, or *ordinal*, or *nominal* scales.\n",
    "Let us illustrate our method for each of these types of categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interval** variables have ordered values, equal distances between values, but the value itself is not necessarily meaningful, for example, 0 does not always mean a complete absence of a quality. The common examples of interval variables are Likert scales:\n",
    "<br>\n",
    "***\n",
    "\"How likely is the person to buy a smartphone mobile phone?\" \n",
    "<br>\n",
    "1 = Very Unlikely <br>\n",
    "2 = Somewhat Unlikely<br>\n",
    "3 = Neither Likely, Nor Unlikely<br>\n",
    "4 = Somewhat Likely<br>\n",
    "5 = Very Likely\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a straightforward way, if we simply use the raw values 1 through 5, that will save us dimensionality without losing a single bit of information. Indeed, the algorithm such as *xgboost* will make splits on this scale to introduce a new node any number of times it finds the best, instead of being forced to use the splits predetermined by One-Hot. With a large K, the algorithm would be forced into using big number of predermined splits which might be simply an overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ordinal** variables have ordered values that are meaningless, and the distances between values are also not equal or not even explainable. An example: <br>\n",
    "***\n",
    "\"What is the highest level of Education completed by the person?\"\n",
    "<br>\n",
    "1 = No High School<br>\n",
    "2 = High School <br>\n",
    "3 = Associate Degree <br>\n",
    "4 = Bachelor's Degree <br>\n",
    "5 = Master's Degree <br>\n",
    "6 = Doctoral Degree <br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to interval variables, ordinal numeric codes can be used in the model without introducing any challenges, as long as the order is correct. In some cases, a variable is intrinsically ordinal, but the given numeric codes are not in the correct order - in these situations we could simply reorder categories and then use this updated variable as a quantitative feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interval and ordinal categorical variables are probably not raising concerns as they are clearly perfect alternative to One-Hot for tree-based algorithms. The more complicated and non-obvious question is how to treat nominal variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nominal** variables have no obvious order between categories, and are almost always encoded with One-Hot method. A nice example of a nominal variable is\n",
    "***\n",
    "\"Color\" <br>\n",
    "(For instance, which color marker a person have chosen in some psychological state)\n",
    "<br>\n",
    "A = Red <br>\n",
    "B = Blue <br>\n",
    "C = Green <br>\n",
    "D = Yellow <br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first glance it seems like we can't convert this variable to a quantitative scale. This is where our proposed method will help! We suggest **finding a rainbow**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seemingly unordered categories acquire a perfect order if we find a certain scale where each category fits into a unique place. With the color, such scale could be the Hue! In other words, the wavelength of the light is the factor the helps us order the colors into a perfect scale - a rainbow! So, following that logic, we would engineer a new feature:\n",
    "***\n",
    "1 = Blue <br>\n",
    "2 = Green<br>\n",
    "3 = Yellow<br>\n",
    "4 = Red<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hue is not the only way to order the colors. We could think of a few different scales, such as brightness, saturation, color temperature, lightness, etc. We invite you to experiment with a few different \"rainbows\" that might capture different nuances of the categorical quality. <br>Note that by using a rainbow rather than One-Hot we are not losing any of the signal, because, even if the categories are wildly different and each one introduces a substantial gain to the model, the algorithm such as *xgboost* would capture that by making all the needed splits when introducing new tree nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets us show a few other examples of creating rainbows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\"Marital Status\" <br>\n",
    "A = Married <br>\n",
    "B = Single <br>\n",
    "C = Inferred Married <br>\n",
    "D = Inferred Single <br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think about Single and Married as the two ends of the spectrum, then Inferred Single could be between the two, closer to Single while Inferred Married between the two closer to Married. That would make sense because being Inferred holds some degree of uncertainty. Thus, the following order would be reasonable:\n",
    "***\n",
    "1 = Single <br>\n",
    "2 = Inferred Single <br>\n",
    "3 = Inferred Married <br>\n",
    "4 = Married\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case there are any missing values here, the Unknown category fits exactly in the middle between Single and Married as there is no reason to prefer one end over the other. So the modified scale could look like this:\n",
    "***\n",
    "1 = Single <br>\n",
    "2 = Inferred Single <br>\n",
    "3 = Unknown <br>\n",
    "3 = Inferred Married <br>\n",
    "4 = Married\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider another variable:\n",
    "***\n",
    "\"Occupation\" <br>\n",
    "1 = Professional/Technical<br>\n",
    "2 = Administration/Managerial<br>\n",
    "3 = Sales/Service<br>\n",
    "4 = Clerical/White Collar<br>\n",
    "5 = Craftsman/Blue Collar<br>\n",
    "6 = Student<br>\n",
    "7 = Homemaker<br>\n",
    "8 = Retired<br>\n",
    "9 = Farmer<br>\n",
    "A = Military<br>\n",
    "B = Religious<br>\n",
    "C = Self Employed<br>\n",
    "D = Other<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a rainbow here might be harder, but here are a few ways to do it - we could order occupations by average annual salary, by its prevalence in the geographic area of interest, or by <font color='red'>Dmytro, add smth</font>. That might involve calling a Census API or some other data source, and might be complicated by the fact that these values are not static, but they are still viable solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if there is no natural rainbow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some situations though we cannot find a logical order for the rainbow because the variable itself is not interpretable. An example could be a black box column made by a third party:\n",
    "***\n",
    "\"Financial Cluster\" <br>\n",
    "1 = Market Watchers <br>\n",
    "2 = Conservative Wealth <br>\n",
    "3 = Specific Savers <br>\n",
    "4 = Tried and True <br>\n",
    "5 = Trendy Inclinations <br>\n",
    "6 = Current Consumers <br>\n",
    "7 = Rural Trust <br>\n",
    "8 = City Spotlight <br>\n",
    "9 = Career Conscious <br>\n",
    "10 = Digital Financiers <br>\n",
    "11 = Financial Futures <br>\n",
    "12 = Stable Influentials <br>\n",
    "13 = Conservatively Rural\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we might not have a clear idea how to order categories due to lack of knowledge of what each category entails. What to do with such variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend creating an artificial rainbow by looking at how each category is related to the target variable. In case of a binary classification problem we have a binary target variable, and we could construct a rainbow at least two different ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First way is to place categories in the order of correlation with target variable. So the category with the highest value of correlation with the dependent variable would acquire numeric code 1, and the category with the lowest correlation would acquire numeric code 13. So our rainbow in this case would mean the relationship between the cluster and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second way is similar to the first one, but instead of correlation, we could look at the percent of target variable taking value of 1 (positive) given each of the categories. Suppose, among Market Watchers percent of positive targets is 0.67, while for Conservative Wealth it is 0.45. In that case, Market Watchers will be ordered higher than Conservative Wealth (or lower, if the target percent scale is ascending). In other words, this rainbow would reflect the prevalence of positive target inside each category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of multiclass classification, we could create rainbows for each class to represent relationship between categories and each class separately. In the case of regression and continuous target, we could rely on the correlation method mostly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We briefly described the Rainbow method, and below we provide theoretical justification for it and then an empirical application that illustrates its superiority over One-Hot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to a Real Data Science Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will illustrate the effectiveness of the rainbow encoding method using the real data science project developed in the Customer Journey domain of Data Science group at MassMutual - a life insurance company with a team of over 200 top-notch data scientists, engineers, and technologists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, the data task is a multiclass classification problem that aims to predict one of the five Mindset Segments for each of the prospective customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Segment | Description |\n",
    "| :- | :- |\n",
    "| Self-Assured |Confident, in control, and middle-aged, often with families|\n",
    "| Juggler | (>= 35 years old) Younger families, less confident in their finances|\n",
    "| Starter |(< 35 years old) Youngest segment, with lower levels of financial confidence due to lack of experience|\n",
    "| Day to Day | Older, worried, and pessimistic about their finances |\n",
    "| Well-Established | In control, and highly satisfied with financial situation |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentation framework represents five classes that reflect a person's age, financial stability, and attitude towards financial decisions. The predicted segments are then used by marketers in different types of campaigns for targeting and customization. For example, Self-Assured customers would value more independence and autonomy in making decision of buying a life insurance policy whereas Day to Day customers would value having a guidance and a long and thorough explanations of different financial products by a dedicated advisor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true segment labels come from MassMutual vendors that ran the mindset survey in 2017, 2018, and 2020. The total size of data is 17.5K rows. The main database we use for this problem is provided by Acxiom and covers about 300 columns representing rich set of demographic characteristics, composition of the household, income and net worth, financial behavior, and digital savvyness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e70e961b5c2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#hide_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msize_of_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_train'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msize_of_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "\n",
    "size_of_data = int((t['n_train'] + t['n_test'])[0])\n",
    "size_of_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Acxiom data and the Mindset Segmentation prediction task, we will compare the conventional One-Hot encoding with the Rainbow method. For a task of 5-class classification we will demonstrate the following metrics - accuracy, Cohen's kappa, <font color='red'>and anything else - Dmytro? Do we want many metrics just for the sake of illustration?</font>\n",
    "<br>\n",
    "Cohen's Kappa is one the best metrics for an unbalanced multiclass classification problem. Accuracy is used for a simple interpretation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we took all existing categorical variables - interval, ordinal, and nominal, and excluded any other variables - quantitative and binary. We would like to compare the model performance using the two types of encoding for the same set of catogorical factors. <br><br>\n",
    "We then applied a target stratified 4-fold Cross Validation split. All the data processing from this point on is done inside the cross validation loop, i.e. the creation of One-Hot features and Rainbow features is learned from each fold train set and applied to each fold validation set. <br> <br>\n",
    "The total set of 111 variables was transformed into **201** Rainbow features and, alternatively, into **2260** One-Hot features (with very slight deviations in N in 4 different folds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Type of variable | N raw | N Rainbow encoded | N One-Hot encoded |\n",
    "| :-:| :-: | :-: | :-:|\n",
    "| Interval |64| 64 | 1670 |\n",
    "| Ordinal | 14 | 14 | 178 |\n",
    "| Nominal |33| 123 | 412 |\n",
    "| Total | 111 | **201** | **2260** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Nominal Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can notice that number of raw and Rainbow encoded features is the same (64 and 14), while 33 raw nominal features turned into 123 Rainbow features. That is because interval and ordinal features have a straightforward rainbow tranformation whereas there were two kinds of nominal variables. Out of 33 nominal variables, for 23 we found a natural rainbow, while for 10 variables we applied correlation ordering and target percent ordering. Since we deal with 5 classes, we made 10 new features for each of these variables.\n",
    "For example, given the name of feature \"Financial_Cluster\" and 5 segment names, we made features\n",
    "- Financial_Cluster_Self-Assured_correlation_rank\n",
    "- Financial_Cluster_Juggler_correlation_rank\n",
    "- Financial_Cluster_Starter_correlation_rank\n",
    "- Financial_Cluster_Day to Day_correlation_rank\n",
    "- Financial_Cluster_Well-Establishes_correlation_rank\n",
    "<br><br> and <Br><br>\n",
    "- Financial_Cluster_Self-Assured_target_percent\n",
    "- Financial_Cluster_Juggler_target_percent\n",
    "- Financial_Cluster_Starter_target_percent\n",
    "- Financial_Cluster_Day to Day_target_percent\n",
    "- Financial_Cluster_Well-Establishes_target_percent\n",
    " <br><br>\n",
    "In this way, 33 raw nominal variables turned into 123 Rainbows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the search for natural or non-natural Rainbows is highly project and context specific, and is more of an art than a craft. For instance, for a binary classification problem, there probably would have been only one or two Rainbows for each categorical feature given a single target class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS CODE WILL BE HIDDEN, OUTPUT WILL BE SHOWN - probably, as a manual mardown table\n",
    "t[t['feature_group']=='all'].groupby(['transform_parameter'])[['accuracy', 'kappa']].mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Probably, this table should be beautified - index corrected, names capitalized and made better. <br>Suggest this to be a manual markdown table rather than code output. <br>Also - other metrics can be added if needed.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the average metrics across all models are notably higher for Rainbow encoding. The following plots show dynamic of metrics depending on every hyperparameter. These plots also clearly demonstrate the superiority of Rainbow method for every hyperparameter and every metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS CODE WILL BE HIDDEN, OUTPUT WILL BE SHOWN - probably, as saved images\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def make_plot(t, feature_group, hyperparameter, hyperparameter_name, metric, ylimit_bottom, ylimit_top, xticks_list):\n",
    "    k1 = pd.DataFrame(t[t['feature_group']==feature_group][t['method']=='rainbow'].groupby(\n",
    "        hyperparameter)[metric].mean().round(3))\n",
    "    k2 = pd.DataFrame(t[t['feature_group']==feature_group][t['method']=='one_hot'].groupby(\n",
    "        hyperparameter)[metric].mean().round(3))\n",
    "    fig, ax = plt.subplots(figsize=(4,3))\n",
    "    plt.plot(k1.index, k1[metric], label = 'Rainbow')\n",
    "    plt.plot(k2.index, k2[metric], label = 'One-Hot')\n",
    "    plt.ylim(ylimit_bottom, ylimit_top)\n",
    "    ax.set_xticks(xticks_list)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(hyperparameter_name)\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.title(metric.capitalize())\n",
    "    plt.show()\n",
    "\n",
    "t=results\n",
    "make_plot(t, 'all', 'n_trees', 'Number of trees', 'kappa', 0.15, 0.21, [50, 100 ,200])\n",
    "make_plot(t, 'all', 'n_trees', 'Number of trees', 'accuracy', 0.39, 0.43, [50, 100 ,200])\n",
    "make_plot(t, 'all', 'eta', 'Learning Rate', 'kappa', 0.15, 0.21, [0.1, 0.3 ,0.5])\n",
    "make_plot(t, 'all', 'eta', 'Learning Rate', 'accuracy', 0.39, 0.43, [0.1, 0.3 ,0.5])\n",
    "make_plot(t, 'all', 'max_depth', 'Max Depth', 'kappa', 0.15, 0.21, [2, 3 ,5])\n",
    "make_plot(t, 'all', 'max_depth', 'Max Depth', 'accuracy', 0.39, 0.43, [2, 3, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>This should probably be a 2*3 plots matrix. <br> Also, other metrics? <br> Also, combinations of hyperparameters? Like max_depth =2, num_trees = 50, eta = 0.5 - the simplest model compare rainbow vs one_hot? <br> Any graphic representaiton of that? <br>\n",
    "Any other ideas, Dmytro?\n",
    "<br>\n",
    "Maybe beautifying charts in some way? Other colors?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's compare the runtime for each method. Average model running time is about 9 times faster for Rainbow method than for One-Hot. So, in addition to a substantial increase in model performance metrics, Rainbow method can save data scientists huge amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS CODE WILL BE HIDDEN, OUTPUT WILL BE SHOWN - probably, as a manual mardown table\n",
    "pd.DataFrame(t[t['feature_group']=='all'].groupby(['method'])['total_time'].mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Same. Correct the table the necessary way</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interval, Ordinal, and Nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the results of the models that applied two types of encoding to interval, ordinal, and nominal features separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS CODE WILL BE HIDDEN, OUTPUT WILL BE SHOWN - probably, as a manual mardown table\n",
    "t[t['feature_group']!='all'].groupby(['feature_group','method'])[['accuracy', 'kappa']].mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Same. Correct the table the necessary way. <br>\n",
    "Also, reorder: interval, ordinal, nominal.<br><br>\n",
    "Add charts if needed <br>\n",
    "Maybe, do something to show that kappa is better for all cases - some magic with nominal</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, interval and ordinal features gain the most from Rainbow encoding, while nominal variables - less so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to make a comparison more fair in terms of dimensionality, we picked top 10, top 50, and top 100 features from each feature set - Rainbow and One-Hot. We used feature importance attribute of the XGBoost model and aggregated feature importance scores for 4 cross validation folds on the best hyperparameter set for each encoding type. Below are the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "\n",
    "table1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*weighted in the table above is class-weighted.\n",
    "\n",
    "Kappa is significantly higher for the Rainbow method, especially with the lower number of selected features. As mentioned before rainbow with 50 features is better than one-hot with 100 features, we just see this that is true in all the regularly used metrics.  Note the drop for macro-f1 score drop from 50 to 10 features in one-hot versus rainbow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "#import numpy as np\n",
    "#np.log(table1[1::2]).diff().iloc[1]/np.log(table1[::2]).diff().iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the Rainbow method is an elegant and simple way to encode categorical variables, that will significantly reduce the data dimensionality without losing any part of valuable signal, that will likely cause substantial improvements in model performance metrics (or, at the very least, will not cause any reductions in metrics), and that will save great amount of time for modelers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we should note that this article just opens the conversation about the Rainbow method, and by no means exhausts the topic. In the potential future investigations, we could explore some other aspects. To name a few: binary and continuous target variables; comparison with other dimensionality reduction methods, such as PCA; how missing values fit the Rainbow framework; is there any limit and implications of combining seemingly unrelated features into a single Rainbow. We hope to open the gate for further questions and feedback on this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Technical Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with models, we applied basic feature selection to each feature set:\n",
    "* Dropped variables with all values missing\n",
    "* No imputations was done – missingness is handled by XGBoost (see [XGBoost is not black magic](https://towardsdatascience.com/xgboost-is-not-black-magic-56ca013144b4))\n",
    "* Droped variables with variance being exactly 0\n",
    "* Dropped perfect duplicates\n",
    "* Dropped perfect rank duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These procedures reduced our Rainbow feature set and One-Hot feauture set to **183** and **2095** respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran all possible XGBoost multiclass classification models covering this space:\n",
    "```\n",
    "'objective': 'multi:softprob'\n",
    "'eval_metric': 'mlogloss'\n",
    "'num_class': 5\n",
    "'subsample': 0.8\n",
    "'max_depth': [2, 3, 5]\n",
    "'eta': [0.1, 0.3, 0.5]\n",
    "'n_estimators': [50, 100, 200]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we ran 3 max_depth * 3 eta * 3 n_estimators * 4 folds * 2 encoding methods = 216 models. Below we report average cross validation metrics for both encoding methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
