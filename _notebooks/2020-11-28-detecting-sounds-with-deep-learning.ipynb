{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting sounds with Deep Learning\n",
    "> ''\n",
    "\n",
    "- author: <a href=http://kozistr.tech/about>Hyeongchan Kim</a>\n",
    "- categories: [python, data science]\n",
    "- image: _notebooks/_assets/resnest-architecture.jpg\n",
    "- permalink: /audio/\n",
    "- hide: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you ever woken up without understanding what it was, but knowing for sure that some sound isn’t right?\n",
    "\n",
    "Sound identification is one of our instincts that kept human beings safe. Sounds play a significant role in our life,\n",
    "Starting from recognizing a predator nearby to being inspired by music, to lots of human voices, to the cry of a bird.\n",
    "Therefore, developing audio classifiers is a crucial task in our lives.\n",
    "\n",
    "In many cases, it is crucial to classify the source of the sounds and is already widely used for various purposes.\n",
    "In music, there's a classifier for the genre of music. Recently similar systems started to be used to classify bird calls, something that historically was done by a profession called Ornithologists.\n",
    "Their goal is to categorize which sounds of birds because it is difficult to detect the birdcalls from the fields or noisy environments.\n",
    "\n",
    "Recently, deep learning (DL) has become one of the popular technologies to solve lots of tasks in our lives due to its accuracy, along with the improvement of computational devices like CPU (Central Processing Unit), GPU (Graphics Processing Unit).\n",
    "The below chart shows how big the deep learning market is and the expected size of its future from the aspects of the software, hardware, and services."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[![](_assets/deep-learning-market.png)](https://www.grandviewresearch.com/industry-analysis/deep-learning-market)\n",
    "\n",
    "<center> Fig 2. Deep Learning market of U.S. from 2014 to 2025 </center>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, We will take the task of reading an audio file with zero to few bird calls and use deep learning to identify which bird it is, based on the [Cornell Birdcall Identification Kaggle Challenge](https://www.kaggle.com/c/birdsong-recognition/data) where we’ve got a silver medal."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to deal with the data?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous post our team wrote, we explained how to load sound data and get it to a spectrogram format and why it is crucial.\n",
    "Here’s an example of a spectrogram of birdcalls of [Alder Flycatcher](https://ebird.org/species/aldfly) and a photo of such a bird, just in case you are curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](_assets/log-mel-spectrogram.png)\n",
    "\n",
    "<center> Fig 3. log mel spectrogram of birdcall, Alder Flycatcher </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed of data processing is one of the keys to utilizing a deep learning model. Although the increment of computation power, the computation cost of audio processing is still expensive on CPUs.\n",
    "However, if we choose a better computation resource to process the data like GPUs, it can boost the speed of about ten to one hundred times faster!\n",
    "In this post, we will show how to process Spectrogram fast by utilizing a library called torchlibrosa that enables us to process Spectrogram on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build Spectrogram processor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[torchlibrosa](https://github.com/qiuqiangkong/torchlibrosa) is a Python library that has some audio processing functions implemented in [PyTorch](https://pytorch.org/) that can utilize GPU resources. PyTorch enables running this Spectrogram algorithm on a GPU. Here's an example of extracting Spectrogram features using torchlibrosa."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchlibrosa.stft import Spectrogram\n",
    "\n",
    "spectrogram_extractor = Spectrogram(win_length=1024, hop_length=320).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load audio data via [librosa](https://librosa.org/doc/latest/index.html) library, which is one of the popular Python audio processing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zero\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\librosa\\core\\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'example.wav'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32mc:\\users\\zero\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\librosa\\core\\audio.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001B[0m\n\u001B[0;32m    145\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 146\u001B[1;33m         \u001B[1;32mwith\u001B[0m \u001B[0msf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSoundFile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0msf_desc\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    147\u001B[0m             \u001B[0msr_native\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msf_desc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msamplerate\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\zero\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\soundfile.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001B[0m\n\u001B[0;32m    628\u001B[0m                                          format, subtype, endian)\n\u001B[1;32m--> 629\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_file\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_open\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode_int\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclosefd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    630\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0missuperset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'r+'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseekable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\zero\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\soundfile.py\u001B[0m in \u001B[0;36m_open\u001B[1;34m(self, file, mode_int, closefd)\u001B[0m\n\u001B[0;32m   1183\u001B[0m         _error_check(_snd.sf_error(file_ptr),\n\u001B[1;32m-> 1184\u001B[1;33m                      \"Error opening {0!r}: \".format(self.name))\n\u001B[0m\u001B[0;32m   1185\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mmode_int\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_snd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSFM_WRITE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\zero\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\soundfile.py\u001B[0m in \u001B[0;36m_error_check\u001B[1;34m(err, prefix)\u001B[0m\n\u001B[0;32m   1356\u001B[0m         \u001B[0merr_str\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_snd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msf_error_number\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1357\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprefix\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0m_ffi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstring\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merr_str\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'utf-8'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'replace'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1358\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error opening 'example.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-bddcfc58ba10>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# get raw audio data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mexample\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlibrosa\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'example.wav'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m32000\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmono\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\zero\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\librosa\\core\\audio.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001B[0m\n\u001B[0;32m    161\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpathlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPurePath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    162\u001B[0m             \u001B[0mwarnings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwarn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"PySoundFile failed. Trying audioread instead.\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 163\u001B[1;33m             \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msr_native\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m__audioread_load\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moffset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mduration\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    164\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    165\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mexc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\zero\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\librosa\\core\\audio.py\u001B[0m in \u001B[0;36m__audioread_load\u001B[1;34m(path, offset, duration, dtype)\u001B[0m\n\u001B[0;32m    185\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    186\u001B[0m     \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 187\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0maudioread\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maudio_open\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0minput_file\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    188\u001B[0m         \u001B[0msr_native\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput_file\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msamplerate\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    189\u001B[0m         \u001B[0mn_channels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput_file\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchannels\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\zero\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\audioread\\__init__.py\u001B[0m in \u001B[0;36maudio_open\u001B[1;34m(path, backends)\u001B[0m\n\u001B[0;32m    109\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mBackendClass\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mbackends\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 111\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mBackendClass\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    112\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mDecodeError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    113\u001B[0m             \u001B[1;32mpass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\zero\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\audioread\\rawread.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, filename)\u001B[0m\n\u001B[0;32m     60\u001B[0m     \"\"\"\n\u001B[0;32m     61\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 62\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fh\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'rb'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     63\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'example.wav'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# get raw audio data\n",
    "example, _ = librosa.load('example.wav', sr=32000, mono=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Process `Spectrogram`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "raw_audio = torch.Tensor(example).unsqueeze(0).cuda()\n",
    "\n",
    "spectrogram = spectrogram_extractor(raw_audio)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Benchmark processing speed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can process audio data on the GPU by using torchlibrosa library. You may wonder how much faster on the GPU than the CPU.\n",
    "Here's the speed of processing the benchmark between the devices. We just selected audio from the dataset obtained from the [Cornell Birdcall Identification Kaggle Challenge](https://www.kaggle.com/c/birdsong-recognition/data),\n",
    "which is publicly available, and compared how long it takes on CPU and GPU.\n",
    "We tested on the [Colab environment](https://colab.research.google.com/) to reproduce the performance, and it is about x15 faster on GPU than CPU to process log-mel spectrogram from about 5 minutes audio."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](_assets/speed-benchmark.png)\n",
    "\n",
    "<center> Fig 4. Processing time between CPU (Intel Xeon 2.20 GHz) and GPU (Nvidia T4). librosa is used for CPU benchmark, torchlibrosa is used for GPU benchmark </center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to classify a sound?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned above, deep learning also shows a brilliant performance in the audio domain. It can catch various patterns of target classes nicely in the time-series data.\n",
    "The more important thing is the environment and data matter in bird calls. The environments like fields or the middle of the mountains, there are lots of noises interfering with the birdcalls.\n",
    "There are lots of birds that can exist in long recorded audio. So considering these cases, we need to build a noise-robust, multi-label audio classifier.\n",
    "\n",
    "We are going to introduce a deep learning architecture used by [our team](https://www.kaggle.com/c/birdsong-recognition/leaderboard) (Dragonsong) in Cornell Bird Call Identification Kaggle Challenge."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Architecture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We built a novel audio classifier architecture that catches time-series features effectively by utilizing [CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network), [RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network) and attention modules.\n",
    "Here is our brief plot of architecture used at the Cornell Birdcall Identification Challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![](_assets/our-architecture.png)\n",
    "\n",
    "<center> Fig 5. Our architecture of birdcall classifier </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We process raw audio with a log-mel spectrogram as an input of our architecture, and it passes through the [ResNeSt50](https://arxiv.org/abs/2004.08955) backbone, which is one of the image classification architectures.\n",
    "Then, we take the features, which contain both spatial and temporal information, to the [RoI](https://en.wikipedia.org/wiki/Region_of_interest) (Region of Interest) pooling and bi-[GRU](https://en.wikipedia.org/wiki/Gated_recurrent_unit) layers.\n",
    "In the layers, it catches the time-wise information again while reducing the feature dimension because we thought of extracting temporal features are crucial to classify lots of bird calls in long audio.\n",
    "Lastly, we pass the information into the attention module to score by each time step to find out which time step the birds exist."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not only building deep learning architecture to represent the data but also how to train the model is crucial (a.k.a training recipe).\n",
    "To classify audios that contain multiple bird calls in a noisy environment, we mix multiple bird calls into audio and noises like white noise.\n",
    "Also, regarding lots of variation of bird calls, we augment time and pitch and mask some audio frames by using [SpecAugment](https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html).\n",
    "Here is a short example of what we applied augmentations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](_assets/mixed.wav)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center> Fig 6. Augmented sample. The mixed version of Alder Flycatcher and American Avocet. </center>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a result, we can achieve an outperform score on the Kaggle challenge."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}