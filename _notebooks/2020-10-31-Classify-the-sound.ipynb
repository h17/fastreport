{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to classify a sound?\n",
    "> ''\n",
    "\n",
    "- author: <a href=http://kozistr.tech/about>Hyeongchan Kim</a>\n",
    "- categories: [python, data science]\n",
    "- image: images/yomex-owo-Bm7Vm8T4BQs-unsplash.jpg\n",
    "- permalink: /audio/\n",
    "- hide: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tons of various sounds exist in our life. Starting from music that many of us usually listen to, lots of human voices, to the cry of endangered species like a bird. In some cases, it is crucial that classifying the source of the sounds and is already widely used for various purposes. Such as, there's a classifier for the genre of music in many music recommendation systems. Even ornithologists try to use a classifier to categorize which sounds of birds because it is hard to detect the birdcalls from the fields or noisy environments. Therefore, audio classifiers play a significant role in our lives.\n",
    "\n",
    "Recently, by the advance of the technologies, computation power is exponentially increased, so even doing heavy computation is possible in a millisecond like deep learning (DL). And it brings promising performance gain over the domains. It enables us to detect the categories more precisely than in the past. However, there's still an issue utilizing deep learning fast and accurately.\n",
    "\n",
    "In this post, We will use the birdcalls as an example and show how to deal with the data fast and detect the rare kind of birds using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Recent works have shown that using convolutional neural networks (CNNs) is powerful in various types of machine learning tasks like classification tasks across domains. CNN enables networks to catch informative features by fusing spatial and channel-wise information within local receptive fields at each layer. For audio signals, lots of related works take a spectrogram as input using CNN that allows extracting features both frequency and time wisely via its kernel, and these approaches show promising results.\n",
    "Unlike sound classification tasks, sound event detection (SED) is a task to detect the events in recorded audio, and it's crucial to find the various length of single or multiple events at a segment-level. Although CNN is a powerful method to extract its feature maps, it can't capture long time dependency in audio due to the limited sizes of the receptive field. To resolve this problem, we utilize both a gated recurrent unit (GRU) and a self-attention mechanism to handle long time dependency effectively. In this article, we'll explain the CNN architecture for detecting sound events by adopting the bi-GRU and self-attention module with the implementation and show performances compared with different methods.~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous post that `Tony` wrote, he explains well why and how we should process the audio data into *Spectrogram* to give to a deep learning model as an input. Continuing from the post, the speed of data processing is one of the keys to utilizing a deep learning model. Although the increment of computation power, the computation cost of audio processing is still expensive on a Central Processing Unit ([CPU](https://en.wikipedia.org/wiki/Central_processing_unit)). However, if we choose a better computation resource to process the data like a Graphic Processing Unit ([GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)), it can boost the speed of about ten to one hundred times faster! In this post, we will show how to process *Spectrogram* fast by utilizing a library called [`torchlibrosa`](https://github.com/qiuqiangkong/torchlibrosa) that enables us to process `Spectrogram` on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchlibrosa` is a `Python` library that has some audio processing functions implemented in [`PyTorch`](https://pytorch.org/) that can utilize `GPU` resources. By implementing the `Spectrogram` algorithm in `PyTorch` which is a deep learning framework, it can run via a GPU device. Here's an example of extracting *Spectrogram* features from the fiven raw audio using `torchlibrosa`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Spectrogram processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchlibrosa.stft import Spectrogram\n",
    "\n",
    "class AudioProcessing(nn.Module):\n",
    "    def __init__(self, window_size: int = 1024, hop_size: int = 320):\n",
    "        super().__init__()\n",
    "        self.spectrogram_extractor = Spectrogram(\n",
    "            hop_length=hop_size, \n",
    "            win_length=window_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.spectrogram_extractor(x)\n",
    "\n",
    "# building processor\n",
    "processor = AudioProcessing().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can lightly load audio data via `librosa` library, which is one of the popular `Python` audio processing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "# get raw audio data\n",
    "example, _ = librosa.load('example.wav', sr=32000, mono=True)\n",
    "example.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Process `Spectrogram` via `torchlibrosa`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spectrogram = processor(torch.Tensor(example).unsqueeze(0).cuda())\n",
    "spectrogram.size()  # (batch_size, 1 (=mono), time_steps, freq_bins)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's it! like the above example code, we can process `Spectrogram` fast by utilizing `torchlibrosa` successfully. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Benchmark processing speed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can check that we can process audio data on the GPU by using torchlibrosa library. Also you may wonder how much faster on the GPU. Here's the speed of processing the benchmark between CPU and GPU devices. We just selected randomly about 1,000 audios from `UrbanSound` dataset, which is publicly available and compare how long it takes on `CPU` and `GPU`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "root_path = 'D:\\\\DataSet\\\\UrbanSound\\\\audio\\\\fold1'\n",
    "audio_paths = sorted(glob(os.path.join(root_path, '*.wav')))\n",
    "\n",
    "raw_audios = [librosa.load(audio_path, sr=32000, mono=True)[0] for audio_path in audio_paths]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_spectrogram(x):\n",
    "    stft = librosa.stft(x, n_fft=1024, hop_length=320)\n",
    "    spectrogram = np.abs(stft)\n",
    "    return spectrogram\n",
    "\n",
    "for raw_audio in raw_audios:\n",
    "    get_spectrogram(raw_audio)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "for raw_audio in raw_audios:\n",
    "    processor(torch.Tensor(raw_audio).unsqueeze(0).cuda())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* show how long is diff\n",
    "* change the dataset to birdcall"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Learning Architecture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned above, deep learning also shows a brilliant performance in the audio domain. It can catch various patterns of target classes nicely in the time-series data. The more important thing is the environment and data matter in birdcalls. The environments like fields or the middle of the mountains, there are lots of noises interfering with the birdcalls. There are lots of birds that can exist in long recorded audio. So considering these cases, we need to build a noise-robust, multi-label classifier.\n",
    "\n",
    "We are going to introduce a deep learning architecture designed in Cornel Birdcall Identification Kaggle Challenge."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Arhcitecture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72000,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process `Spectrogram` via `torchlibrosa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 226, 1025])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrogram = processor(torch.Tensor(example).unsqueeze(0).cuda())\n",
    "spectrogram.size()  # (batch_size, 1 (=mono), time_steps, freq_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! like the above example code, we can process `Spectrogram` fast by utilizing `torchlibrosa` successfully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark processing speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that we can process audio data on the GPU by using torchlibrosa library. Also you may wonder how much faster on the GPU. Here's the speed of processing the benchmark between CPU and GPU devices. We just selected randomly about 1,000 audios from `UrbanSound` dataset, which is publicly available and compare how long it takes on `CPU` and `GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "root_path = 'D:\\\\DataSet\\\\UrbanSound\\\\audio\\\\fold1'\n",
    "audio_paths = sorted(glob(os.path.join(root_path, '*.wav')))\n",
    "\n",
    "raw_audios = [librosa.load(audio_path, sr=32000, mono=True)[0] for audio_path in audio_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_spectrogram(x):\n",
    "    stft = librosa.stft(x, n_fft=1024, hop_length=320)\n",
    "    spectrogram = np.abs(stft)\n",
    "    return spectrogram\n",
    "\n",
    "for raw_audio in raw_audios:\n",
    "    get_spectrogram(raw_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for raw_audio in raw_audios:\n",
    "    processor(torch.Tensor(raw_audio).unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* show how long is diff\n",
    "* change the dataset to birdcall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, deep learning also shows a brilliant performance in the audio domain. It can catch various patterns of target classes nicely in the time-series data. The more important thing is the environment and data matter in birdcalls. The environments like fields or the middle of the mountains, there are lots of noises interfering with the birdcalls. There are lots of birds that can exist in long recorded audio. So considering these cases, we need to build a noise-robust, multi-label classifier.\n",
    "\n",
    "We are going to introduce a deep learning architecture designed in Cornel Birdcall Identification Kaggle Challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arhcitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}